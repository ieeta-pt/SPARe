{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagoalmeida/safe_volume/syn-question-col-analysis/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-06 16:18:59.130375: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-06 16:18:59.706228: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-06 16:18:59.706271: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-06 16:18:59.706278: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import psutil\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import (Magics, magics_class, line_magic,\n",
    "                                cell_magic, line_cell_magic)\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic('ram_profiler')\n",
    "def ram_profiler(line, cell):\n",
    "    ram_before = psutil.virtual_memory().used / 1024 / 1024 / 1024\n",
    "    exec(cell,  globals())\n",
    "    ram_now = psutil.virtual_memory().used / 1024 / 1024 / 1024\n",
    "    print(f\"Ram Profiler | Ram diff: {ram_now-ram_before:.4f} GB\")\n",
    "    #return line, cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(hf_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_ram():\n",
    "    try:\n",
    "        ram_info = psutil.virtual_memory()\n",
    "        print(f\"Total: {ram_info.total / 1024 / 1024 / 1024:.2f} GB\")\n",
    "        print(f\"Available: {ram_info.available / 1024 / 1024 / 1024:.2f} GB\")\n",
    "        print(f\"Used: {ram_info.used / 1024 / 1024 / 1024:.2f} GB\")\n",
    "        print(f\"Percentage usage: {ram_info.percent}%\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Ram info not available on this system\")\n",
    "\n",
    "def get_bow(text, tokenizer):\n",
    "    bow = defaultdict(float)\n",
    "    for t in tokenizer(text, add_special_tokens=False).input_ids:\n",
    "        bow[t]+=1.0\n",
    "    return bow\n",
    "\n",
    "def get_bow_tensor(text, tokenizer, dense=False, dtype=torch.float):\n",
    "    bow = get_bow(text, tokenizer)\n",
    "    #bow = Counter(tokenizer(text, add_special_tokens=False).input_ids)\n",
    "    indices = list(bow.keys())\n",
    "    indices2d = [indices,[0]*len(indices)]\n",
    "    values = list(bow.values())\n",
    "    if dense:\n",
    "        return torch.sparse_coo_tensor(indices2d, values, (tokenizer.vocab_size, 1), dtype=dtype).to_dense()\n",
    "    else:\n",
    "        \n",
    "        return torch.sparse_coo_tensor(indices2d, values, (tokenizer.vocab_size, 1), dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document collection\n",
    "PATH_TO_MSMARCO = \"../syn-question-col-analysis/datasets/msmarco/corpus_L8841823.jsonl\"\n",
    "\n",
    "def get_title_abstract(string):\n",
    "    data = json.loads(string)\n",
    "    return data[\"title\"], data[\"abstract\"]\n",
    "\n",
    "SIZE = 1_000_000\n",
    "#batch_i = 0\n",
    "#batched_collection = [[]]\n",
    "\n",
    "def load_collection(SIZE):\n",
    "    collections = []\n",
    "    with open(PATH_TO_MSMARCO) as f:\n",
    "        \n",
    "        for i,(title, abstract) in enumerate(tqdm(map(get_title_abstract,f), total=SIZE)):\n",
    "            collections.append(get_bow_tensor(f\"{title} {abstract}\", hf_tokenizer))\n",
    "            if i>SIZE:\n",
    "                break\n",
    "    \n",
    "    return collections\n",
    "\n",
    "def load_collection_to_tensor(path_to_collection, size=8841823, dtype=torch.float32):\n",
    "\n",
    "    batch_size = 250_000\n",
    "    batch_i = 0\n",
    "    _temp_collections = []\n",
    "    sparse_collections = None\n",
    "    with open(path_to_collection) as f:\n",
    "        \n",
    "        for i,(title, abstract) in enumerate(tqdm(map(get_title_abstract,f), total=size)):\n",
    "            \n",
    "            if i>=size:\n",
    "                break\n",
    "            \n",
    "            sparse_vec = get_bow_tensor(f\"{title} {abstract}\", hf_tokenizer, dtype=dtype)\n",
    "            \n",
    "            if sparse_collections is None:\n",
    "                sparse_collections = sparse_vec\n",
    "            else:\n",
    "                if batch_i<batch_size:\n",
    "                    batch_i +=1\n",
    "                    _temp_collections.append(sparse_vec)\n",
    "                else:\n",
    "                    batch_i = 0\n",
    "                    _temp_collections.append(sparse_vec)\n",
    "                    sparse_collections = torch.concat([sparse_collections]+_temp_collections, dim=-1)\n",
    "                    _temp_collections = []\n",
    "                    gc.collect()\n",
    "            \n",
    "            #collections.append(get_bow_tensor(f\"{title} {abstract}\", hf_tokenizer))\n",
    "\n",
    "        sparse_collections = torch.concat([sparse_collections]+_temp_collections, dim=-1)\n",
    "        _temp_collections = []\n",
    "        gc.collect()\n",
    "    \n",
    "    return sparse_collections\n",
    "\n",
    "def load_collection_to_coo(path_to_collection, hf_tokenizer, size=8841823, dtype=torch.float32):\n",
    "    \n",
    "    def get_max_values_for_coo_format(mem_available, num_bytes_per_value = 4):\n",
    "        overhead = 1 * 1e6\n",
    "        return int((mem_available-overhead)/(16+num_bytes_per_value))\n",
    "    \n",
    "    indice_row = []\n",
    "    indice_col = []\n",
    "    values = []\n",
    "    \n",
    "    with open(path_to_collection) as f:\n",
    "        \n",
    "        for i,(title, abstract) in enumerate(tqdm(map(get_title_abstract,f), total=size)):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 508916/1000000 [02:39<02:11, 3723.30it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "1000001it [05:11, 3206.68it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram Profiler | Ram diff: 1.3721 GB\n"
     ]
    }
   ],
   "source": [
    "%%ram_profiler\n",
    "\n",
    "sparse_collection = load_collection_to_tensor(PATH_TO_MSMARCO, size=1_000_000,dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   1996,    3739,    1997,  ...,    2011,    2039,\n",
       "                           2322],\n",
       "                       [      0,       0,       0,  ..., 1000000, 1000000,\n",
       "                        1000000]]),\n",
       "       values=tensor([6., 1., 5.,  ..., 1., 1., 1.]),\n",
       "       size=(30522, 1000001), nnz=47905032, dtype=torch.float16,\n",
       "       layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015719783578982478"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "95959559/(30522*1999994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram Profiler | Ram diff: 0.8775 GB\n"
     ]
    }
   ],
   "source": [
    "%%ram_profiler\n",
    "\n",
    "sparse_concat_collection = torch.concat(collections, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram Profiler | Ram diff: 0.9530 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:54.)\n"
     ]
    }
   ],
   "source": [
    "%%ram_profiler\n",
    "sparse_collection = sparse_concat_collection.to_sparse_csr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram Profiler | Ram diff: -0.8960 GB\n"
     ]
    }
   ],
   "source": [
    "%%ram_profiler\n",
    "del sparse_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram Profiler | Ram diff: -0.8960 GB\n"
     ]
    }
   ],
   "source": [
    "%%ram_profiler\n",
    "del sparse_concat_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram Profiler | Ram diff: -0.0021 GB\n"
     ]
    }
   ],
   "source": [
    "%%ram_profiler\n",
    "collections = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ram_profiler\n",
    "\n",
    "del sparse_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[2498],\n",
       "                       [   0]]),\n",
       "       values=tensor([1.]),\n",
       "       size=(30522, 1), nnz=1, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tensor(\"nothing\", hf_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_bow_tensor(\"this is nothing\", hf_tokenizer)\n",
    "b = get_bow_tensor(\"this is nothing LOL\", hf_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_Counter_class(l_tokens):\n",
    "    bow_c = Counter(l_tokens)\n",
    "    return list(bow_c.keys()), list(bow_c.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = list(bow.keys())\n",
    "indices2d = torch.tensor([indices,[0]*len(indices)])\n",
    "\n",
    "values = torch.tensor(list(bow.values()), dtype=torch.float)\n",
    "\n",
    "target = torch.zeros([hf_tokenizer.vocab_size,1], dtype=torch.float)  \n",
    "target.index_put_(tuple([k for k in indices2d]), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not dict_values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49msparse_coo_tensor(indices2d, bow\u001b[39m.\u001b[39;49mvalues(), (\u001b[39m60_000\u001b[39;49m,\u001b[39m1\u001b[39;49m), dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16)\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not dict_values"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.sparse_coo_tensor(indices2d, bow.values(), (60_000,1), dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syn-quest-venv",
   "language": "python",
   "name": "syn-quest-venv"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98f543f2b62ee20a9d441d9f3b19468d98f3bfe9156394279428e70bbfddb556"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
