{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collection import SparseCollection, SparseCollectionCSR\n",
    "from backend import TYPE\n",
    "import json\n",
    "import psutil\n",
    "from text2vec import BagOfWords\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from transformations import BM25Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(hf_tokenizer.vocab_size)\n",
    "\n",
    "\n",
    "bow = BagOfWords(hf_tokenizer, hf_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MSMARCO = \"../syn-question-col-analysis/datasets/msmarco/corpus_L8841823.jsonl\"\n",
    "\n",
    "def get_title_abstract(string):\n",
    "    data = json.loads(string)\n",
    "    title, abstract = data[\"title\"], data[\"abstract\"]\n",
    "    return f\"{title} {abstract}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Size estimation: 100%|██████████| 1000/1000 [00:00<00:00, 4858.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We estimate that the collection matrix will have density of 0.0023, which requires 0.557624 GB. Plus 0.5GB for overheads.\n",
      "Expected number of elements 69203000 for a shape (1000000, 30522)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sparse matrix:   1%|          | 8555/999000 [00:02<04:25, 3736.36it/s]"
     ]
    }
   ],
   "source": [
    "with open(PATH_TO_MSMARCO) as f:\n",
    "    collection_iterator = map(get_title_abstract,f)\n",
    "    \n",
    "    sparseCSR_collection = SparseCollectionCSR.from_text_iterator(collection_iterator,\n",
    "                                                             collection_maxsize=1_000_000,#8841823,\n",
    "                                                             text_to_vec=bow,\n",
    "                                                             dtype=TYPE.float32,\n",
    "                                                             indices_dtype=TYPE.int32,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to BM25 matrix: 100%|██████████| 1000000/1000000 [00:36<00:00, 27678.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([       0,       40,       67,  ..., 47904868, 47904922, 47904978],\n",
       "        dtype=torch.int32),\n",
       " tensor([ 1012,  1025,  1996,  ..., 10651, 10995, 15728], dtype=torch.int32),\n",
       " tensor([0.0214, 3.0450, 0.2211,  ..., 7.6188, 5.6052, 9.8994]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparseCSR_collection.transform(BM25Transform())\n",
    "sparseCSR_collection.sparse_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparseCSR_collection.save_to_file(\"csr_msmarco.safetensors\")\n",
    "# load tensors\n",
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"csr_msmarco.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "   for key in f.keys():\n",
    "        tensors[key] = f.get_tensor(key)\n",
    "       \n",
    "tensors = [tensors[\"vec_0\"], tensors[\"vec_1\"], tensors[\"vec_2\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_938608/1569663226.py:1: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:54.)\n",
      "  csr_matrix_cpu = torch.sparse_csr_tensor(*tensors, (8841823, 30522))\n"
     ]
    }
   ],
   "source": [
    "csr_matrix_cpu = torch.sparse_csr_tensor(*tensors, (8841823, 30522))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_dense_torch(text, dim):\n",
    "    b = bow(text)\n",
    "    return torch.sparse_coo_tensor([list(b.keys())], list(b.values()), (dim,), dtype=torch.float32).to_dense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_matrix_gpu = csr_matrix_cpu.to(\"cuda:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_QUESTIONS = \"../syn-question-col-analysis/datasets/msmarco/relevant_pairs.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(PATH_TO_QUESTIONS) as f:\n",
    "    questions = {line[\"question\"] for line in map(json.loads,f)}\n",
    "\n",
    "questions = [text_to_dense_torch(q, csr_matrix_cpu.shape[1]) for q in questions]\n",
    "queries_gpu = torch.stack(questions[:20], -1).to(\"cuda:1\")\n",
    "\n",
    "#sort, idx = (csr_matrix_gpu @ queries_gpu).sort(descending=True, dim=0)\n",
    "#result = idx[:10].cpu()\n",
    "result = torch.topk(csr_matrix_gpu @ queries_gpu, k=10, dim=0).indices.T.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8617271, 7607669, 5466810, 1379245, 1379240, 5466807, 1664523, 8617274,\n",
       "          547444,  269428],\n",
       "        [1929910, 3572695, 7839904, 1288938, 4842897, 2507917, 7839906, 3572702,\n",
       "         5359212, 3937200],\n",
       "        [2533260, 5012351, 8121380,  719552, 5291683, 7952865, 5291686,  719550,\n",
       "         6528714, 7088568],\n",
       "        [8049577, 1433123,  669004, 4563960, 1584254, 3410067, 2055598, 8049578,\n",
       "           16845, 3552218],\n",
       "        [8635981, 7267248,  527698, 3260688, 1837110, 1958102, 1958100, 7267243,\n",
       "         8199361, 7367407],\n",
       "        [8760867, 8760864, 3641634, 2787508, 4788864, 2157456, 8760868, 8760873,\n",
       "         3620983, 3342992],\n",
       "        [7778351, 2868845, 6436703, 7778348, 4164404, 4337532, 2197526, 2997653,\n",
       "         7670593, 2160853],\n",
       "        [7447941, 8433858, 6654655, 8433854, 7896211, 2747492, 2365660, 5638740,\n",
       "         4704978, 2702419],\n",
       "        [8160520, 3838645,  554521, 4511137,  398442, 4575877, 5218014, 8160527,\n",
       "         1901881, 8478604],\n",
       "        [ 536176, 3705165, 2978866, 5653659, 2329697, 4511503, 4606387, 5653652,\n",
       "         1006868, 1006859],\n",
       "        [1445087, 8446502, 1798612, 6177939, 5699286, 8093934, 8446505, 8714082,\n",
       "         5752877, 3349612],\n",
       "        [5674622, 8255705,  190809, 8160241, 1883281, 3953805, 6467522, 4443375,\n",
       "         5674623, 7093509],\n",
       "        [1749882, 8819113, 7965004, 6018163, 8081937, 2026777, 7843984, 8819111,\n",
       "          186939, 4634899],\n",
       "        [ 576629,   68647,   68653,   68648, 2293820,  943229,   68649, 2826510,\n",
       "         3223833, 3223831],\n",
       "        [8128798, 6021898, 8128790, 4981632, 3997986, 8128796, 1066532, 8128794,\n",
       "         7618564, 4486954],\n",
       "        [8273755, 8273763, 7941579, 8273762, 8273759, 6031630, 2998723, 8273757,\n",
       "         8273754, 8185064],\n",
       "        [7911557, 8588219, 8588222, 8588226, 2697752, 2697746, 8588227,  128984,\n",
       "         2176863,  302210],\n",
       "        [1334335, 5836920, 8348781, 7726655, 8084332, 3537614, 8199524, 2143418,\n",
       "         2777108, 2777113],\n",
       "        [ 115142, 8683095, 6230226, 1524406, 2702357, 3437288, 6230227, 1524402,\n",
       "         1969741, 1900577],\n",
       "        [8117094, 8117087, 6704400, 8117093, 7196961, 8117092, 6704401, 5279567,\n",
       "         8327123, 5139286]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del queries_gpu\n",
    "del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_list = []\n",
    "for _ in range(20):\n",
    "    \n",
    "    with open(PATH_TO_QUESTIONS) as f:\n",
    "        questions = {line[\"question\"] for line in map(json.loads,f)}\n",
    "\n",
    "    questions = [text_to_dense_torch(q, csr_matrix_cpu.shape[1]) for q in questions]\n",
    "    queries_gpu = torch.stack(questions, -1).to(\"cuda:1\")\n",
    "\n",
    "    start_t = time.time()\n",
    "    #sort, idx = (csr_matrix_gpu @ queries_gpu).sort(descending=True, dim=0)\n",
    "    #result = idx[:10].cpu()\n",
    "    result = torch.topk(csr_matrix_gpu @ queries_gpu, k=100000, dim=0).indices.cpu()\n",
    "    time_list.append(time.time()-start_t)\n",
    "    \n",
    "    for q in questions:\n",
    "        del q\n",
    "    del questions\n",
    "    del queries_gpu\n",
    "    del result \n",
    "    #del sort\n",
    "    #del idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4787881374359131,\n",
       " 0.44898533821105957,\n",
       " 0.4485352039337158,\n",
       " 0.4484899044036865,\n",
       " 0.44817113876342773,\n",
       " 0.4484434127807617,\n",
       " 0.4484429359436035,\n",
       " 0.44898128509521484,\n",
       " 0.44836854934692383,\n",
       " 0.44843435287475586,\n",
       " 0.4483344554901123,\n",
       " 0.4485807418823242,\n",
       " 0.44898390769958496,\n",
       " 0.44857192039489746,\n",
       " 0.44869017601013184,\n",
       " 0.448455810546875,\n",
       " 0.44840002059936523,\n",
       " 0.4485960006713867,\n",
       " 0.4486415386199951,\n",
       " 0.4485054016113281]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_matrix_gpu = csr_matrix_cpu.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [text_to_dense_torch(\"what is the meaning of life?\", csr_matrix_cpu.shape[1]),\n",
    "           text_to_dense_torch(\"what time it is?\", csr_matrix_cpu.shape[1]),\n",
    "            ]\n",
    "\n",
    "queries = torch.stack(queries, -1)\n",
    "queries_gpu = queries.to(\"cuda:1\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.3629, 3.6962],\n",
       "        [0.5774, 0.0000],\n",
       "        [0.5461, 1.5242],\n",
       "        ...,\n",
       "        [0.5942, 0.0000],\n",
       "        [0.5746, 2.5969],\n",
       "        [1.1826, 0.5707]], device='cuda:1')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time_list = []\n",
    "for _ in range(20):\n",
    "    \n",
    "    queries = [text_to_dense_torch(\"what is the meaning of life?\", csr_matrix_cpu.shape[1]),\n",
    "           text_to_dense_torch(\"what time it is?\", csr_matrix_cpu.shape[1]),\n",
    "           text_to_dense_torch(\"what time it is? 2\", csr_matrix_cpu.shape[1]),\n",
    "           text_to_dense_torch(\"wagfsdf asdg asg ?\", csr_matrix_cpu.shape[1]),\n",
    "           #text_to_dense_torch(\"wwef faes ewq ta afewf s?\", csr_matrix_cpu.shape[1]),\n",
    "            ]\n",
    "\n",
    "    queries = torch.stack(queries, -1)\n",
    "    queries_gpu = queries.to(\"cuda:1\")\n",
    "\n",
    "    start_t = time.time()\n",
    "    csr_matrix_gpu @ queries_gpu\n",
    "    time_list.append(time.time()-start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08742809295654297"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_list)/len(time_list)*1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(PATH_TO_QUESTIONS) as f:\n",
    "    questions = {line[\"question\"] for line in map(json.loads,f)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_gpu = queries.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.42 GiB (GPU 1; 7.79 GiB total capacity; 6.26 GiB already allocated; 1.19 GiB free; 6.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m queries_gpu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([text_to_dense_torch(q, csr_matrix_cpu\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m questions], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m start_t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m result \u001b[39m=\u001b[39m csr_matrix_gpu \u001b[39m@\u001b[39;49m queries_gpu\n\u001b[1;32m     12\u001b[0m time_list\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart_t)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.42 GiB (GPU 1; 7.79 GiB total capacity; 6.26 GiB already allocated; 1.19 GiB free; 6.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1820206642150879"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_list)/len(time_list)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%timeit -n 1000\n",
    "retrieve_gpu_nvprims_nvfuser(query_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.3629, 0.5774, 0.5461,  ..., 0.5942, 0.5746, 1.1826], device='cuda:1')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_gpu = query.to(\"cuda:1\")\n",
    "retrieve_gpu4(csr_matrix_gpu, query_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 ms ± 78.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -l 1000\n",
    "retrieve_gpu4(csr_matrix_gpu, query_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 ms ± 88 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "csr_matrix @ query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 ms ± 4.17 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.topk(csr_matrix@query,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([     0,     40,     67,  ..., 465841, 465881,\n",
       "                            465917]),\n",
       "       col_indices=tensor([ 1012,  1025,  1996,  ...,  9949, 17502, 26110]),\n",
       "       values=tensor([2., 1., 6.,  ..., 1., 1., 3.]), size=(10000, 30522),\n",
       "       nnz=465917, layout=torch.sparse_csr)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to BM25 matrix: 100%|██████████| 10000/10000 [00:00<00:00, 27992.58it/s]\n"
     ]
    }
   ],
   "source": [
    "sparseCSR_collection.transform(BM25Transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([     0,     40,     67,  ..., 465841, 465881, 465917],\n",
       "        dtype=torch.int32),\n",
       " tensor([ 1012,  1025,  1996,  ...,  9949, 17502, 26110], dtype=torch.int32),\n",
       " tensor([0.0000, 2.1513, 0.0000,  ..., 6.4540, 8.6054, 9.9709]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparseCSR_collection.sparse_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([     0,     40,     67,  ..., 465841, 465881,\n",
       "                            465917]),\n",
       "       col_indices=tensor([ 1012,  1025,  1996,  ...,  9949, 17502, 26110]),\n",
       "       values=tensor([0.0000, 2.1513, 0.0000,  ..., 6.4540, 8.6054, 9.9709]),\n",
       "       size=(10000, 30522), nnz=465917, layout=torch.sparse_csr)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sparse_csr_tensor(*sparseCSR_collection.sparse_vecs, sparseCSR_collection.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-sparse",
   "language": "python",
   "name": "gpu-sparse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
