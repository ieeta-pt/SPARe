{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiagoalmeida/safe_volume/syn-question-col-analysis/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-09-06 16:37:29.489948: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-06 16:37:30.049759: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-06 16:37:30.049801: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-06 16:37:30.049807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import psutil\n",
    "\n",
    "from utils import get_bytes_in_dtype, get_coo_space_GB, get_csr_or_csc_space_GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic('ram_profiler')\n",
    "def ram_profiler(line, cell):\n",
    "    ram_before = psutil.virtual_memory().used / 1024 / 1024 / 1024\n",
    "    exec(cell,  globals())\n",
    "    ram_now = psutil.virtual_memory().used / 1024 / 1024 / 1024\n",
    "    print(f\"Ram Profiler | Ram diff: {ram_now-ram_before:.4f} GB\")\n",
    "    #return line, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(hf_tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow(text, tokenizer):\n",
    "    bow = defaultdict(float)\n",
    "    for t in tokenizer(text, add_special_tokens=False).input_ids:\n",
    "        bow[t]+=1.0\n",
    "    return bow\n",
    "\n",
    "PATH_TO_MSMARCO = \"../syn-question-col-analysis/datasets/msmarco/corpus_L8841823.jsonl\"\n",
    "\n",
    "def get_title_abstract(string):\n",
    "    data = json.loads(string)\n",
    "    return data[\"title\"], data[\"abstract\"]\n",
    "\n",
    "\n",
    "#batch_i = 0\n",
    "#batched_collection = [[]]\n",
    "\n",
    "def get_matrix_estimations(collection_iterator, \n",
    "                      collection_maxsize, \n",
    "                      hf_tokenizer, \n",
    "                      dtype=torch.float32, \n",
    "                      max_files_for_estimation=1000):\n",
    "    \n",
    "    list_bow = []\n",
    "    for _ in range(max_files_for_estimation):\n",
    "        title, abstract = next(collection_iterator)\n",
    "        list_bow.append(get_bow(f\"{title} {abstract}\", hf_tokenizer))\n",
    "\n",
    "    dense_size = hf_tokenizer.vocab_size*len(list_bow)\n",
    "    density = sum([sum(bow.values()) for bow in list_bow])/dense_size\n",
    "    shape = (hf_tokenizer.vocab_size, collection_maxsize)\n",
    "    \n",
    "    return shape, density, list_bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coo_collection(collection_path, \n",
    "                          collection_maxsize, \n",
    "                          hf_tokenizer, \n",
    "                          dtype=torch.float32, \n",
    "                          max_files_for_estimation=1000):\n",
    "    \n",
    "    with open(collection_path) as f:\n",
    "        collection_iterator = map(get_title_abstract,f)\n",
    "        \n",
    "        shape, density, list_bow = get_matrix_estimations(collection_iterator, collection_maxsize, hf_tokenizer, dtype, max_files_for_estimation)\n",
    "        \n",
    "        mem_needed = get_coo_space_GB(shape=shape, density=density,dtype=dtype)\n",
    "        elements_expected = int(shape[0] * shape[1] * density)\n",
    "        print(f\"We estimate that the collection matrix will have density of {density:.4f}, which requires {mem_needed} GB. Plus 0.5GB for overheads.\")\n",
    "        \n",
    "        # make a verification if it fits the GPU mem and CPU! plus add strategies if doesnt\n",
    "        print(f\"Expected number of elements {elements_expected} for a shape {shape}\")\n",
    "        \n",
    "        ## creating the tensors\n",
    "        indices = torch.zeros((2,elements_expected), dtype=torch.int64)\n",
    "        values = torch.zeros((elements_expected,), dtype=dtype)\n",
    "        \n",
    "        element_index = 0\n",
    "        colum_index = 0\n",
    "        ## add values for the current processed documents\n",
    "        for bow in list_bow:\n",
    "            for token_index in sorted(bow.keys()):\n",
    "                \n",
    "                indices[0,element_index] = token_index\n",
    "                indices[1,element_index] = colum_index\n",
    "                values[element_index] = bow[token_index]\n",
    "                \n",
    "                element_index+=1\n",
    "            colum_index+=1\n",
    "            del bow\n",
    "        \n",
    "        del list_bow\n",
    "        \n",
    "        index_docs = 0\n",
    "        \n",
    "        for title, abstract in tqdm(collection_iterator, total=collection_maxsize-max_files_for_estimation):\n",
    "            \n",
    "            if index_docs>=(collection_maxsize-max_files_for_estimation):\n",
    "                break\n",
    "            \n",
    "            bow = get_bow(f\"{title} {abstract}\", hf_tokenizer)\n",
    "            py_indices_row = []\n",
    "            py_indices_col = []\n",
    "            py_values = []\n",
    "            for token_index in sorted(bow.keys()):\n",
    "                py_indices_row.append(token_index)\n",
    "                py_indices_col.append(colum_index)\n",
    "                py_values.append(bow[token_index])\n",
    "                \n",
    "            indices[:,element_index:element_index+len(py_indices_row)] = torch.tensor([py_indices_row,py_indices_col], dtype=torch.int64)\n",
    "            values[element_index:element_index+len(py_indices_row)] = torch.tensor(py_values, dtype=dtype)\n",
    "            colum_index+=1\n",
    "            element_index+=len(py_indices_row)\n",
    "            index_docs+=1\n",
    "    # narrow\n",
    "    indices = indices[:,:element_index]\n",
    "    values = values[:element_index] \n",
    "    \n",
    "    \n",
    "    return torch.sparse_coo_tensor(indices, values, shape)\n",
    "        #for title, abstract in collection_iterator:\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We estimate that the collection matrix will have density of 0.0023, which requires 1.107248 GB. Plus 0.5GB for overheads.\n",
      "Expected number of elements 69203000 for a shape (30522, 1000000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999000/999000 [04:17<00:00, 3884.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram Profiler | Ram diff: 0.0832 GB\n"
     ]
    }
   ],
   "source": [
    "%%ram_profiler\n",
    "\n",
    "space_coo = create_coo_collection(PATH_TO_MSMARCO, 1_000_000, hf_tokenizer) #1000 its/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[  1012,   1025,   1996,  ...,  10651,  10995,  15728],\n",
       "                       [     0,      0,      0,  ..., 999999, 999999, 999999]]),\n",
       "       values=tensor([2., 1., 6.,  ..., 3., 1., 2.]),\n",
       "       size=(30522, 1000000), nnz=47904978, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1996, 1997, 2003, 2015, 2023, 2033, 2507, 2546, 4933, 14383, 24646]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted(bow.keys()) 47904904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 46247])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor( [[1],[2]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syn-quest-venv",
   "language": "python",
   "name": "syn-quest-venv"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
