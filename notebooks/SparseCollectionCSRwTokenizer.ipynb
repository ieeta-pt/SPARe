{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.9.2 has loaded Terrier 5.7 (built by craigm on 2022-11-10 18:30) and terrier-helper 0.0.7\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "from collection import SparseCollection, SparseCollectionCSR\n",
    "from backend import TYPE\n",
    "import json\n",
    "import psutil\n",
    "from text2vec import BagOfWords\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyterrier  as pt\n",
    "import pandas as pd\n",
    "pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:37:23.660 [main] WARN org.terrier.structures.BaseCompressingMetaIndex - Structure meta reading data file directly from disk (SLOW) - try index.meta.data-source=fileinmem in the index properties file. 1.9 GiB of memory would be required.\n"
     ]
    }
   ],
   "source": [
    "indexref = pt.IndexRef.of(\"../syn-question-col-analysis/datasets/msmarco/terrier_index/\")\n",
    "index = pt.IndexFactory.of(indexref)\n",
    "\n",
    "def tp_func():\n",
    "    stops = pt.autoclass(\"org.terrier.terms.Stopwords\")(None)\n",
    "    stemmer = pt.autoclass(\"org.terrier.terms.PorterStemmer\")(None)\n",
    "    def _apply_func(row):\n",
    "        words = row[\"query\"].split(\" \") # this is safe following pt.rewrite.tokenise()\n",
    "        words = [stemmer.stem(w) for w in words if not stops.isStopword(w) ]\n",
    "        return words\n",
    "    return _apply_func \n",
    "\n",
    "pipe = pt.rewrite.tokenise() >> pt.apply.query(tp_func())\n",
    "token2id = {word.getKey():i for i,word in enumerate(index.getLexicon()) }\n",
    "\n",
    "vocab_size = len(index.getLexicon())\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens_ids = []\n",
    "    for token in pipe(pd.DataFrame([{\"qid\":0, \"query\":text.lower()}]))[\"query\"][0]:\n",
    "        if token in token2id:\n",
    "            token_id=token2id[token]\n",
    "            if token_id is not None:\n",
    "                tokens_ids.append(token_id)\n",
    "    return tokens_ids\n",
    "\n",
    "bow = BagOfWords(tokenizer, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_collection = SparseCollectionCSR.load_from_file(\"beir_datasets/msmarco/csr_anserini_bm25_12_075\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2191770/2760414820.py:1: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  csr_matrix_cpu = torch.sparse_csr_tensor(*sparse_collection.sparse_vecs, sparse_collection.shape)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "csr_matrix_cpu = torch.sparse_csr_tensor(*sparse_collection.sparse_vecs, sparse_collection.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /home/tiagoalmeida/.local/lib/python3.10/site-packages/pytorch_quantization/cuda_ext.cpython-310-x86_64-linux-gnu.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_quantization\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_quant\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_quantization/__init__.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabsl\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mquant_modules\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     22\u001b[0m logging\u001b[39m.\u001b[39muse_absl_handler()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_quantization/quant_modules.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcontextlib\u001b[39;00m \u001b[39mimport\u001b[39;00m contextmanager\n\u001b[1;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_quantization\u001b[39;00m \u001b[39mimport\u001b[39;00m nn \u001b[39mas\u001b[39;00m quant_nn\n\u001b[1;32m     25\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39minitialize\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdeactivate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39menable_onnx_export\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[39m# Definition of the named tuple that is used to store mapping of the quantized modules\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_quantization/nn/__init__.py:19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# SPDX-License-Identifier: Apache-2.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_quantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensor_quantizer\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_quantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquant_conv\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_quantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquant_linear\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_quantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensor_quant\u001b[39;00m \u001b[39mimport\u001b[39;00m QuantDescriptor, tensor_quant, fake_tensor_quant, scaled_e4m3\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_quantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclip\u001b[39;00m \u001b[39mimport\u001b[39;00m Clip\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_quantization\u001b[39;00m \u001b[39mimport\u001b[39;00m calib\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytorch_quantization/tensor_quant.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_onnx\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_C_onnx\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m \u001b[39mimport\u001b[39;00m Function\n\u001b[0;32m---> 28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_quantization\u001b[39;00m \u001b[39mimport\u001b[39;00m cuda_ext\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39monnx\u001b[39;00m \u001b[39mimport\u001b[39;00m symbolic_helper\n\u001b[1;32m     33\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mScaledQuantDescriptor\u001b[39;00m():\n",
      "\u001b[0;31mImportError\u001b[0m: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /home/tiagoalmeida/.local/lib/python3.10/site-packages/pytorch_quantization/cuda_ext.cpython-310-x86_64-linux-gnu.so)"
     ]
    }
   ],
   "source": [
    "from pytorch_quantization import tensor_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0+cu121'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_matrix_gpu = csr_matrix_cpu.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../syn-question-col-analysis/question_generation/gen_output/msmarco/selected_corpus_lm_fcm_STD2_L10000_gpt-neo-1.3B_BS_5_E13931.459746599197.jsonl\") as f:\n",
    "    questions = [line for line in map(json.loads, f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_dense_torch(text):\n",
    "    b = bow(text)\n",
    "    return torch.sparse_coo_tensor([list(b.keys())], list(b.values()), (vocab_size,), dtype=torch.float32).to_dense()\n",
    "\n",
    "def text_to_sparse_torch(text):\n",
    "    b = bow(text)\n",
    "    return torch.sparse_coo_tensor([list(b.keys()), [0]*len(b)], list(b.values()), (vocab_size,1), dtype=torch.float32)#.to_sparse_csc()\n",
    "\n",
    "\n",
    "def retrieve_topk_gpu(question, at=10):\n",
    "    query_gpu = text_to_dense_torch(question).to(\"cuda:1\")\n",
    "    r = torch.topk(csr_matrix_gpu @ query_gpu, k=at, dim=0).indices#.cpu()\n",
    "    #print(\"tokenizer stack\", end_stack-end_tok_t, \"retrieve\",end_r_t-end_stack)\n",
    "    return r\n",
    "\n",
    "def retrieve_topk_cpu(question, at=10):\n",
    "    query = text_to_dense_torch(question)\n",
    "    r = torch.topk(csr_matrix_cpu @ query, k=at, dim=0).indices.cpu()\n",
    "    #print(\"tokenizer stack\", end_stack-end_tok_t, \"retrieve\",end_r_t-end_stack)\n",
    "    return r\n",
    "\n",
    "\n",
    "def batch_retrieve_topk(questions_text, at=10):\n",
    "    # does not work!\n",
    "    queries_gpu_tensor = torch.zeros(vocab_size, len(questions_text))\n",
    "    end_tok_t = time.time()\n",
    "    for i, question in enumerate(questions_text):\n",
    "        queries_gpu_tensor[:,i] = text_to_dense_torch(question)\n",
    "\n",
    "    queries_gpu_tensor = queries_gpu_tensor.to(\"cuda:1\")\n",
    "\n",
    "    end_stack = time.time()\n",
    "    r = torch.topk(csr_matrix_gpu @ queries_gpu_tensor, k=at, dim=0).indices.T.cpu()\n",
    "    end_r_t = time.time()\n",
    "    #print(\"tokenizer stack\", end_stack-end_tok_t, \"retrieve\",end_r_t-end_stack)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47931 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument mat in method wrapper_SparseCsrCUDA_out_addmv_out)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m start_t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m tqdm(questions):\n\u001b[0;32m----> 6\u001b[0m     results\u001b[39m.\u001b[39mappend(retrieve_topk_gpu(question[\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m], at\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m))\n\u001b[1;32m      7\u001b[0m end_t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn [16], line 12\u001b[0m, in \u001b[0;36mretrieve_topk_gpu\u001b[0;34m(question, at)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mretrieve_topk_gpu\u001b[39m(question, at\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m):\n\u001b[1;32m     11\u001b[0m     query_gpu \u001b[39m=\u001b[39m text_to_dense_torch(question)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     r \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtopk(csr_matrix_gpu \u001b[39m@\u001b[39;49m query_gpu, k\u001b[39m=\u001b[39mat, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mindices\u001b[39m#.cpu()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39m#print(\"tokenizer stack\", end_stack-end_tok_t, \"retrieve\",end_r_t-end_stack)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m r\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument mat in method wrapper_SparseCsrCUDA_out_addmv_out)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "results = []\n",
    "start_t = time.time()\n",
    "for question in tqdm(questions):\n",
    "    results.append(retrieve_topk_gpu(question[\"question\"], at=10))\n",
    "end_t = time.time()\n",
    "\n",
    "#print(1000/(end_t-start_t), \"it/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/748 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument mat2 in method wrapper_SparseCsrCUDA__mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m bq \u001b[39min\u001b[39;00m tqdm(batches_questions):\n\u001b[0;32m----> 6\u001b[0m     results\u001b[39m.\u001b[39mappend(batch_retrieve_topk(bq,\u001b[39m10\u001b[39m))\n",
      "Cell \u001b[0;32mIn [16], line 33\u001b[0m, in \u001b[0;36mbatch_retrieve_topk\u001b[0;34m(questions_text, at)\u001b[0m\n\u001b[1;32m     30\u001b[0m queries_gpu_tensor \u001b[39m=\u001b[39m queries_gpu_tensor\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m end_stack \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 33\u001b[0m r \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtopk(csr_matrix_gpu \u001b[39m@\u001b[39;49m queries_gpu_tensor, k\u001b[39m=\u001b[39mat, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m     34\u001b[0m end_r_t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     35\u001b[0m \u001b[39m#print(\"tokenizer stack\", end_stack-end_tok_t, \"retrieve\",end_r_t-end_stack)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument mat2 in method wrapper_SparseCsrCUDA__mm)"
     ]
    }
   ],
   "source": [
    "# batch retrievel\n",
    "batches_questions = [list(map(lambda x:x[\"question\"], questions[i:i+64])) for i in range(64,len(questions),64)]\n",
    "\n",
    "results = []\n",
    "for bq in tqdm(batches_questions):\n",
    "    results.append(batch_retrieve_topk(bq,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batches_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gpu = text_to_dense_torch(questions[0][\"question\"]).to(\"cuda:0\")\n",
    "r = torch.topk(csr_matrix_gpu @ query_gpu, k=10, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([73.5256, 16.0785, 15.6707, 13.2659, 12.6108, 12.5160, 12.4417, 12.3735,\n",
       "        12.0173, 11.8916], device='cuda:0'),\n",
       "indices=tensor([ 341, 9927,   44, 9688, 4810, 3132, 1989, 1985, 4819, 3131],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1170682)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_collection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {880134: 1.0, 869173: 1.0, 274027: 1.0, 541942: 1.0, 652667: 1.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow(questions[0][\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relationship', 'rarotongan', 'cook', 'island', 'maori']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokens = pipe(pd.DataFrame([{\"qid\":0, \"query\":questions[0][\"question\"].lower()}]))[\"query\"][0]\n",
    "query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MSMARCO = \"../syn-question-col-analysis/datasets/msmarco/corpus_L8841823.jsonl\"\n",
    "\n",
    "def get_title_abstract(string):\n",
    "    data = json.loads(string)\n",
    "    title, abstract = data[\"title\"], data[\"abstract\"]\n",
    "    return f\"{title} {abstract}\"\n",
    "\n",
    "with open(PATH_TO_MSMARCO) as f:\n",
    "    for doc in map(get_title_abstract, f):\n",
    "       \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {839186: 1.0,\n",
       "             266051: 1.0,\n",
       "             927074: 2.0,\n",
       "             684233: 1.0,\n",
       "             373151: 1.0,\n",
       "             523132: 1.0,\n",
       "             998799: 2.0,\n",
       "             651144: 1.0,\n",
       "             845056: 1.0,\n",
       "             533255: 1.0,\n",
       "             767761: 1.0,\n",
       "             257234: 1.0,\n",
       "             474305: 1.0,\n",
       "             523347: 1.0,\n",
       "             77832: 1.0,\n",
       "             140132: 1.0,\n",
       "             884129: 1.0,\n",
       "             367894: 1.0,\n",
       "             1066913: 1.0,\n",
       "             666305: 1.0,\n",
       "             506868: 1.0,\n",
       "             1042085: 1.0,\n",
       "             530548: 1.0,\n",
       "             625474: 1.0,\n",
       "             754559: 1.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "batched_queries = [questions[i:i + batch_size] for i in range(0, len(questions), batch_size)]\n",
    "\n",
    "results = []\n",
    "start_t = time.time()\n",
    "for batch in tqdm(batched_queries):\n",
    "    results.append(batch_retrieve_topk(list(map(lambda x:x[\"question\"], batch))))\n",
    "end_t = time.time()\n",
    "\n",
    "print(\"took\", end_t-start_t)\n",
    "# 0.003 - 0.003\n",
    "# 0.008 - 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gpu = text_to_dense_torch(question[\"question\"]).to(\"cuda:1\")\n",
    "end_stack = time.time()\n",
    "r = torch.topk(csr_matrix_gpu @ query_gpu, k=10, dim=0).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1424203, 2678308, 3748774, 3233037, 2741266, 5938529, 5026090, 5495619,\n",
       "        2542788,  838903], device='cuda:1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.to_sparse_csc>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_dense_torch(text, dim):\n",
    "    b = bow(text)\n",
    "    return torch.sparse_coo_tensor([list(b.keys())], list(b.values()), (dim,), dtype=torch.float32).to_dense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_matrix_gpu = csr_matrix_cpu.to(\"cuda:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_QUESTIONS = \"../syn-question-col-analysis/datasets/msmarco/relevant_pairs.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(PATH_TO_QUESTIONS) as f:\n",
    "    questions = {line[\"question\"] for line in map(json.loads,f)}\n",
    "\n",
    "questions = [text_to_dense_torch(q, csr_matrix_cpu.shape[1]) for q in questions]\n",
    "queries_gpu = torch.stack(questions[:20], -1).to(\"cuda:1\")\n",
    "\n",
    "#sort, idx = (csr_matrix_gpu @ queries_gpu).sort(descending=True, dim=0)\n",
    "#result = idx[:10].cpu()\n",
    "result = torch.topk(csr_matrix_gpu @ queries_gpu, k=10, dim=0).indices.T.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8617271, 7607669, 5466810, 1379245, 1379240, 5466807, 1664523, 8617274,\n",
       "          547444,  269428],\n",
       "        [1929910, 3572695, 7839904, 1288938, 4842897, 2507917, 7839906, 3572702,\n",
       "         5359212, 3937200],\n",
       "        [2533260, 5012351, 8121380,  719552, 5291683, 7952865, 5291686,  719550,\n",
       "         6528714, 7088568],\n",
       "        [8049577, 1433123,  669004, 4563960, 1584254, 3410067, 2055598, 8049578,\n",
       "           16845, 3552218],\n",
       "        [8635981, 7267248,  527698, 3260688, 1837110, 1958102, 1958100, 7267243,\n",
       "         8199361, 7367407],\n",
       "        [8760867, 8760864, 3641634, 2787508, 4788864, 2157456, 8760868, 8760873,\n",
       "         3620983, 3342992],\n",
       "        [7778351, 2868845, 6436703, 7778348, 4164404, 4337532, 2197526, 2997653,\n",
       "         7670593, 2160853],\n",
       "        [7447941, 8433858, 6654655, 8433854, 7896211, 2747492, 2365660, 5638740,\n",
       "         4704978, 2702419],\n",
       "        [8160520, 3838645,  554521, 4511137,  398442, 4575877, 5218014, 8160527,\n",
       "         1901881, 8478604],\n",
       "        [ 536176, 3705165, 2978866, 5653659, 2329697, 4511503, 4606387, 5653652,\n",
       "         1006868, 1006859],\n",
       "        [1445087, 8446502, 1798612, 6177939, 5699286, 8093934, 8446505, 8714082,\n",
       "         5752877, 3349612],\n",
       "        [5674622, 8255705,  190809, 8160241, 1883281, 3953805, 6467522, 4443375,\n",
       "         5674623, 7093509],\n",
       "        [1749882, 8819113, 7965004, 6018163, 8081937, 2026777, 7843984, 8819111,\n",
       "          186939, 4634899],\n",
       "        [ 576629,   68647,   68653,   68648, 2293820,  943229,   68649, 2826510,\n",
       "         3223833, 3223831],\n",
       "        [8128798, 6021898, 8128790, 4981632, 3997986, 8128796, 1066532, 8128794,\n",
       "         7618564, 4486954],\n",
       "        [8273755, 8273763, 7941579, 8273762, 8273759, 6031630, 2998723, 8273757,\n",
       "         8273754, 8185064],\n",
       "        [7911557, 8588219, 8588222, 8588226, 2697752, 2697746, 8588227,  128984,\n",
       "         2176863,  302210],\n",
       "        [1334335, 5836920, 8348781, 7726655, 8084332, 3537614, 8199524, 2143418,\n",
       "         2777108, 2777113],\n",
       "        [ 115142, 8683095, 6230226, 1524406, 2702357, 3437288, 6230227, 1524402,\n",
       "         1969741, 1900577],\n",
       "        [8117094, 8117087, 6704400, 8117093, 7196961, 8117092, 6704401, 5279567,\n",
       "         8327123, 5139286]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del queries_gpu\n",
    "del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_list = []\n",
    "for _ in range(20):\n",
    "    \n",
    "    with open(PATH_TO_QUESTIONS) as f:\n",
    "        questions = {line[\"question\"] for line in map(json.loads,f)}\n",
    "\n",
    "    questions = [text_to_dense_torch(q, csr_matrix_cpu.shape[1]) for q in questions]\n",
    "    queries_gpu = torch.stack(questions, -1).to(\"cuda:1\")\n",
    "\n",
    "    start_t = time.time()\n",
    "    #sort, idx = (csr_matrix_gpu @ queries_gpu).sort(descending=True, dim=0)\n",
    "    #result = idx[:10].cpu()\n",
    "    result = torch.topk(csr_matrix_gpu @ queries_gpu, k=100000, dim=0).indices.cpu()\n",
    "    time_list.append(time.time()-start_t)\n",
    "    \n",
    "    for q in questions:\n",
    "        del q\n",
    "    del questions\n",
    "    del queries_gpu\n",
    "    del result \n",
    "    #del sort\n",
    "    #del idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4787881374359131,\n",
       " 0.44898533821105957,\n",
       " 0.4485352039337158,\n",
       " 0.4484899044036865,\n",
       " 0.44817113876342773,\n",
       " 0.4484434127807617,\n",
       " 0.4484429359436035,\n",
       " 0.44898128509521484,\n",
       " 0.44836854934692383,\n",
       " 0.44843435287475586,\n",
       " 0.4483344554901123,\n",
       " 0.4485807418823242,\n",
       " 0.44898390769958496,\n",
       " 0.44857192039489746,\n",
       " 0.44869017601013184,\n",
       " 0.448455810546875,\n",
       " 0.44840002059936523,\n",
       " 0.4485960006713867,\n",
       " 0.4486415386199951,\n",
       " 0.4485054016113281]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_matrix_gpu = csr_matrix_cpu.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [text_to_dense_torch(\"what is the meaning of life?\", csr_matrix_cpu.shape[1]),\n",
    "           text_to_dense_torch(\"what time it is?\", csr_matrix_cpu.shape[1]),\n",
    "            ]\n",
    "\n",
    "queries = torch.stack(queries, -1)\n",
    "queries_gpu = queries.to(\"cuda:1\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.3629, 3.6962],\n",
       "        [0.5774, 0.0000],\n",
       "        [0.5461, 1.5242],\n",
       "        ...,\n",
       "        [0.5942, 0.0000],\n",
       "        [0.5746, 2.5969],\n",
       "        [1.1826, 0.5707]], device='cuda:1')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time_list = []\n",
    "for _ in range(20):\n",
    "    \n",
    "    queries = [text_to_dense_torch(\"what is the meaning of life?\", csr_matrix_cpu.shape[1]),\n",
    "           text_to_dense_torch(\"what time it is?\", csr_matrix_cpu.shape[1]),\n",
    "           text_to_dense_torch(\"what time it is? 2\", csr_matrix_cpu.shape[1]),\n",
    "           text_to_dense_torch(\"wagfsdf asdg asg ?\", csr_matrix_cpu.shape[1]),\n",
    "           #text_to_dense_torch(\"wwef faes ewq ta afewf s?\", csr_matrix_cpu.shape[1]),\n",
    "            ]\n",
    "\n",
    "    queries = torch.stack(queries, -1)\n",
    "    queries_gpu = queries.to(\"cuda:1\")\n",
    "\n",
    "    start_t = time.time()\n",
    "    csr_matrix_gpu @ queries_gpu\n",
    "    time_list.append(time.time()-start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08742809295654297"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_list)/len(time_list)*1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(PATH_TO_QUESTIONS) as f:\n",
    "    questions = {line[\"question\"] for line in map(json.loads,f)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_gpu = queries.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.42 GiB (GPU 1; 7.79 GiB total capacity; 6.26 GiB already allocated; 1.19 GiB free; 6.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m queries_gpu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([text_to_dense_torch(q, csr_matrix_cpu\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m questions], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m start_t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m result \u001b[39m=\u001b[39m csr_matrix_gpu \u001b[39m@\u001b[39;49m queries_gpu\n\u001b[1;32m     12\u001b[0m time_list\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart_t)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.42 GiB (GPU 1; 7.79 GiB total capacity; 6.26 GiB already allocated; 1.19 GiB free; 6.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1820206642150879"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_list)/len(time_list)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%timeit -n 1000\n",
    "retrieve_gpu_nvprims_nvfuser(query_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.3629, 0.5774, 0.5461,  ..., 0.5942, 0.5746, 1.1826], device='cuda:1')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_gpu = query.to(\"cuda:1\")\n",
    "retrieve_gpu4(csr_matrix_gpu, query_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 ms ± 78.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -l 1000\n",
    "retrieve_gpu4(csr_matrix_gpu, query_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 ms ± 88 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "csr_matrix @ query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 ms ± 4.17 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.topk(csr_matrix@query,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([     0,     40,     67,  ..., 465841, 465881,\n",
       "                            465917]),\n",
       "       col_indices=tensor([ 1012,  1025,  1996,  ...,  9949, 17502, 26110]),\n",
       "       values=tensor([2., 1., 6.,  ..., 1., 1., 3.]), size=(10000, 30522),\n",
       "       nnz=465917, layout=torch.sparse_csr)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to BM25 matrix: 100%|██████████| 10000/10000 [00:00<00:00, 27992.58it/s]\n"
     ]
    }
   ],
   "source": [
    "sparseCSR_collection.transform(BM25Transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([     0,     40,     67,  ..., 465841, 465881, 465917],\n",
       "        dtype=torch.int32),\n",
       " tensor([ 1012,  1025,  1996,  ...,  9949, 17502, 26110], dtype=torch.int32),\n",
       " tensor([0.0000, 2.1513, 0.0000,  ..., 6.4540, 8.6054, 9.9709]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparseCSR_collection.sparse_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([     0,     40,     67,  ..., 465841, 465881,\n",
       "                            465917]),\n",
       "       col_indices=tensor([ 1012,  1025,  1996,  ...,  9949, 17502, 26110]),\n",
       "       values=tensor([0.0000, 2.1513, 0.0000,  ..., 6.4540, 8.6054, 9.9709]),\n",
       "       size=(10000, 30522), nnz=465917, layout=torch.sparse_csr)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sparse_csr_tensor(*sparseCSR_collection.sparse_vecs, sparseCSR_collection.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparse-retrieval",
   "language": "python",
   "name": "sparse-retrieval"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
