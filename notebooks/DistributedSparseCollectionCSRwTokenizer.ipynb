{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.9.2 has loaded Terrier 5.7 (built by craigm on 2022-11-10 18:30) and terrier-helper 0.0.7\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:14:04.245 [main] WARN org.terrier.structures.BaseCompressingMetaIndex - Structure meta reading data file directly from disk (SLOW) - try index.meta.data-source=fileinmem in the index properties file. 1.9 GiB of memory would be required.\n"
     ]
    }
   ],
   "source": [
    "from collection import SparseCollection, SparseCollectionCSR\n",
    "from retriever import SparseRetriever\n",
    "from weighting_model import BM25WeightingModel\n",
    "from backend import TYPE\n",
    "import json\n",
    "import psutil\n",
    "from text2vec import BagOfWords\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import multiprocessing  as mp\n",
    "\n",
    "import pyterrier  as pt\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "pt.init()\n",
    "\n",
    "indexref = pt.IndexRef.of(\"../syn-question-col-analysis/datasets/msmarco/terrier_index/\")\n",
    "index = pt.IndexFactory.of(indexref)\n",
    "\n",
    "def tp_func():\n",
    "    stops = pt.autoclass(\"org.terrier.terms.Stopwords\")(None)\n",
    "    stemmer = pt.autoclass(\"org.terrier.terms.PorterStemmer\")(None)\n",
    "    def _apply_func(row):\n",
    "        words = row[\"query\"].split(\" \") # this is safe following pt.rewrite.tokenise()\n",
    "        words = [stemmer.stem(w) for w in words if not stops.isStopword(w) ]\n",
    "        return words\n",
    "    return _apply_func \n",
    "\n",
    "pipe = pt.rewrite.tokenise() >> pt.apply.query(tp_func())\n",
    "token2id = {word.getKey():i for i,word in enumerate(index.getLexicon()) }\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens_ids = []\n",
    "    for token in pipe(pd.DataFrame([{\"qid\":0, \"query\":text.lower()}]))[\"query\"][0]:\n",
    "        if token in token2id:\n",
    "            token_id=token2id[token]\n",
    "            if token_id is not None:\n",
    "                tokens_ids.append(token_id)\n",
    "    return tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../syn-question-col-analysis/question_generation/gen_output/msmarco/selected_corpus_lm_fcm_STD2_L10000_gpt-neo-1.3B_BS_5_E13931.459746599197.jsonl\") as f:\n",
    "    questions = [line[\"question\"] for line in map(json.loads, f)]\n",
    "\n",
    "sparse_collection = SparseCollectionCSR.load_from_file(\"csr_msmarco_bm25_12_075_terrier\")\n",
    "\n",
    "bow = BagOfWords(tokenizer, sparse_collection.shape[1])\n",
    "\n",
    "class QuestionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, questions, bow, vocab_size):\n",
    "        self.questions = questions#[:10000]\n",
    "        self.bow = bow\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b = self.bow(self.questions[idx])\n",
    "        indices = list(b.keys())\n",
    "        values = list(b.values())\n",
    "        \n",
    "        return indices, values#{\"indices\": indices, \"values\": values}\n",
    "    \n",
    "dataset = QuestionDataset(questions, bow, sparse_collection.shape[1])\n",
    "\n",
    "def distributed_collate_fn(data):\n",
    "    max_len = max([len(x[0]) for x in data])\n",
    "    indices = []\n",
    "    values = []\n",
    "    sizes = []\n",
    "    for x in data:\n",
    "        sizes.append(len(x[0]))\n",
    "        indices.append(x[0]+[0]*(max_len-len(x[0])))\n",
    "        values.append(x[1]+[0]*(max_len-len(x[1])))\n",
    "    return torch.tensor(indices), torch.tensor(values), torch.tensor(sizes)\n",
    "\n",
    "dl = torch.utils.data.DataLoader(dataset, batch_size=2, collate_fn=distributed_collate_fn, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[880134, 869173, 274027, 541942, 652667],\n",
       "         [869173, 274027, 541942, 652667,      0]]),\n",
       " tensor([[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 0.]]),\n",
       " tensor([5, 4])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(dl))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSRSparseRetrievalDistributedModel(torch.nn.Module):\n",
    "    def __init__(self, sparse_collection, top_k = 10):\n",
    "        super().__init__()\n",
    "        #self.shape = sparse_collection.sparse_vecs, sparse_collection.shape\n",
    "        self.crow = torch.nn.parameter.Parameter(sparse_collection.sparse_vecs[0], requires_grad=False)\n",
    "        self.indice = torch.nn.parameter.Parameter(sparse_collection.sparse_vecs[1], requires_grad=False)\n",
    "        self.values = torch.nn.parameter.Parameter(sparse_collection.sparse_vecs[2], requires_grad=False)\n",
    "        self.collection_matrix = None#torch.sparse_csr_tensor(self.crow, self.indice, self.values, sparse_collection.shape)\n",
    "        self.shape = sparse_collection.shape\n",
    "        self.top_k = top_k\n",
    "        \n",
    "    def forward(self, indices, values, size):\n",
    "        size = size.squeeze(0)\n",
    "        values = values[0,:size]\n",
    "        indices = indices[0, :size].unsqueeze(0)\n",
    "        \n",
    "        query = torch.sparse_coo_tensor(indices, values, (self.shape[-1],), dtype=torch.float32).to_dense()\n",
    "        #print(x.shape)\n",
    "        collection_matrix = torch.sparse_csr_tensor(self.crow, self.indice, self.values, self.shape)\n",
    "    \n",
    "        return torch.topk(collection_matrix @ query, k=self.top_k, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model= CSRSparseRetrievalDistributedModel(sparse_collection, top_k=10).to(\"cuda\")\n",
    "\n",
    "replicas = torch.nn.parallel.replicate(sparse_model, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_collection = SparseCollectionCSR.load_from_file(\"csr_msmarco_bm25_12_075_terrier\")\n",
    "\n",
    "bow = BagOfWords(tokenizer, sparse_collection.shape[1])\n",
    "\n",
    "# add the option if no weighitngmodel then use the collection weighting model\n",
    "sparse_retriver = SparseRetriever(sparse_collection, bow, BM25WeightingModel())\n",
    "\n",
    "with open(\"../syn-question-col-analysis/question_generation/gen_output/msmarco/selected_corpus_lm_fcm_STD2_L10000_gpt-neo-1.3B_BS_5_E13931.459746599197.jsonl\") as f:\n",
    "    questions = [line[\"question\"] for line in map(json.loads, f)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/home/tiagoalmeida/safe_volume/sparse-retrieval/backend_torch.py:136: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:54.)\n",
      "  collection_matrix = torch.sparse_csr_tensor(self.crow, self.indice, self.values, self.shape)\n",
      "100%|██████████| 10000/10000 [01:34<00:00, 106.17it/s]\n"
     ]
    }
   ],
   "source": [
    "indices, values = sparse_retriver.retrieve(questions[:10_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices[0]\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_indices = []\n",
    "#values_list = []\n",
    "for i in range(indices.shape[0]):\n",
    "    q_indices = [sparse_collection.metadata.index2docID[idx] for idx in indices[i].tolist()]\n",
    "    converted_indices.append(q_indices)\n",
    "    #values_list.append(values[i].tolist())\n",
    "#converted_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[341,\n",
       " 6080047,\n",
       " 6080050,\n",
       " 6080052,\n",
       " 2787655,\n",
       " 3224336,\n",
       " 4383297,\n",
       " 5254867,\n",
       " 3005847,\n",
       " 8529165,\n",
       " 2684918,\n",
       " 3224335,\n",
       " 4719606,\n",
       " 256282,\n",
       " 4719605,\n",
       " 5970721,\n",
       " 2003533,\n",
       " 760538,\n",
       " 5316812,\n",
       " 8491925,\n",
       " 4265297,\n",
       " 8491926,\n",
       " 7791450,\n",
       " 7103086,\n",
       " 5945756,\n",
       " 1680681,\n",
       " 2788095,\n",
       " 8491927,\n",
       " 943733,\n",
       " 8370577,\n",
       " 4430990,\n",
       " 2757777,\n",
       " 7103261,\n",
       " 8491930,\n",
       " 2788092,\n",
       " 5324897,\n",
       " 8513416,\n",
       " 396979,\n",
       " 8370578,\n",
       " 2003539,\n",
       " 2274925,\n",
       " 4150059,\n",
       " 5616934,\n",
       " 2317632,\n",
       " 2003538,\n",
       " 5945759,\n",
       " 6747148,\n",
       " 2660504,\n",
       " 115228,\n",
       " 8370579,\n",
       " 214475,\n",
       " 473866,\n",
       " 8370581,\n",
       " 7507826,\n",
       " 7006077,\n",
       " 5229958,\n",
       " 3136236,\n",
       " 7941668,\n",
       " 6030992,\n",
       " 943729,\n",
       " 4554884,\n",
       " 2757774,\n",
       " 4559939,\n",
       " 2688278,\n",
       " 115233,\n",
       " 6747146,\n",
       " 546033,\n",
       " 2554568,\n",
       " 6891002,\n",
       " 7941667,\n",
       " 2317636,\n",
       " 5607909,\n",
       " 6614214,\n",
       " 7137291,\n",
       " 8206871,\n",
       " 6326123,\n",
       " 5607908,\n",
       " 4798159,\n",
       " 4853270,\n",
       " 7791456,\n",
       " 6212158,\n",
       " 7618170,\n",
       " 785924,\n",
       " 8491929,\n",
       " 3452685,\n",
       " 8370585,\n",
       " 6708193,\n",
       " 6996099,\n",
       " 2788093,\n",
       " 3041330,\n",
       " 7105622,\n",
       " 8090523,\n",
       " 290039,\n",
       " 6708197,\n",
       " 8023051,\n",
       " 5229960,\n",
       " 6747145,\n",
       " 2684914,\n",
       " 6708190,\n",
       " 6708192,\n",
       " 7137295,\n",
       " 6996102,\n",
       " 6749843,\n",
       " 8386300,\n",
       " 2003532,\n",
       " 4286289,\n",
       " 2685334,\n",
       " 4383294,\n",
       " 3904769,\n",
       " 4554879,\n",
       " 3041327,\n",
       " 3534923,\n",
       " 8090526,\n",
       " 7006082,\n",
       " 261700,\n",
       " 3755317,\n",
       " 3755313,\n",
       " 6615205,\n",
       " 5488205,\n",
       " 6043809,\n",
       " 8799215,\n",
       " 3579248,\n",
       " 7285548,\n",
       " 8806116,\n",
       " 6080054,\n",
       " 7618165,\n",
       " 8296633,\n",
       " 4148425,\n",
       " 2684912,\n",
       " 6996104,\n",
       " 1183679,\n",
       " 7791457,\n",
       " 8142071,\n",
       " 3904772,\n",
       " 7507830,\n",
       " 8529162,\n",
       " 1547659,\n",
       " 1831055,\n",
       " 4359007,\n",
       " 7507828,\n",
       " 760539,\n",
       " 2003535,\n",
       " 3524007,\n",
       " 6708191,\n",
       " 7791452,\n",
       " 7941666,\n",
       " 6413312,\n",
       " 290038,\n",
       " 8142076,\n",
       " 8612586,\n",
       " 8142069,\n",
       " 556953,\n",
       " 699082,\n",
       " 8738151,\n",
       " 6747150,\n",
       " 7105623,\n",
       " 2003531,\n",
       " 3847917,\n",
       " 2554571,\n",
       " 6708198,\n",
       " 7618166,\n",
       " 3595419,\n",
       " 2243799,\n",
       " 3755319,\n",
       " 3208615,\n",
       " 6212160,\n",
       " 2274922,\n",
       " 2688275,\n",
       " 4555584,\n",
       " 3918941,\n",
       " 1237769,\n",
       " 2681464,\n",
       " 7941665,\n",
       " 5616932,\n",
       " 5616935,\n",
       " 214876,\n",
       " 2688276,\n",
       " 3666577,\n",
       " 3920967,\n",
       " 5229961,\n",
       " 5706772,\n",
       " 7103253,\n",
       " 8491924,\n",
       " 6588931,\n",
       " 5379485,\n",
       " 6996101,\n",
       " 4286287,\n",
       " 427198,\n",
       " 7006081,\n",
       " 6768411,\n",
       " 5703866,\n",
       " 6708196,\n",
       " 7006080,\n",
       " 7941664,\n",
       " 7103260,\n",
       " 5966134,\n",
       " 1547660,\n",
       " 943724,\n",
       " 2133431,\n",
       " 2732594,\n",
       " 2460385,\n",
       " 1382110,\n",
       " 3755312,\n",
       " 6413311,\n",
       " 4265295,\n",
       " 3918935,\n",
       " 2972643,\n",
       " 3918938,\n",
       " 6080053,\n",
       " 290035,\n",
       " 3973412,\n",
       " 3040998,\n",
       " 6588930,\n",
       " 427199,\n",
       " 2684915,\n",
       " 3612175,\n",
       " 3303194,\n",
       " 4823217,\n",
       " 2981282,\n",
       " 4286291,\n",
       " 6286065,\n",
       " 7507833,\n",
       " 789589,\n",
       " 3524003,\n",
       " 8612584,\n",
       " 5084877,\n",
       " 8384721,\n",
       " 4570846,\n",
       " 5226649,\n",
       " 3630253,\n",
       " 5431954,\n",
       " 2685333,\n",
       " 1100615,\n",
       " 8612589,\n",
       " 1636761,\n",
       " 4642538,\n",
       " 3904765,\n",
       " 7791453,\n",
       " 8142072,\n",
       " 631322,\n",
       " 2697988,\n",
       " 4555586,\n",
       " 2133432,\n",
       " 2688279,\n",
       " 4555585,\n",
       " 8068812,\n",
       " 3918937,\n",
       " 3666576,\n",
       " 3677684,\n",
       " 214877,\n",
       " 785922,\n",
       " 3904768,\n",
       " 4244748,\n",
       " 4323679,\n",
       " 4153754,\n",
       " 4554878,\n",
       " 7006078,\n",
       " 7941669,\n",
       " 8806113,\n",
       " 1237772,\n",
       " 5229956,\n",
       " 6980498,\n",
       " 6085315,\n",
       " 4650435,\n",
       " 8370393,\n",
       " 1617663,\n",
       " 2903825,\n",
       " 214878,\n",
       " 2550637,\n",
       " 2684913,\n",
       " 6620096,\n",
       " 5455426,\n",
       " 5455428,\n",
       " 7560399,\n",
       " 6975321,\n",
       " 1636759,\n",
       " 6043806,\n",
       " 6588929,\n",
       " 5609881,\n",
       " 3918936,\n",
       " 2426286,\n",
       " 7960302,\n",
       " 65162,\n",
       " 958603,\n",
       " 1183676,\n",
       " 3331015,\n",
       " 4554875,\n",
       " 5229957,\n",
       " 6545629,\n",
       " 6996106,\n",
       " 7006079,\n",
       " 7941672,\n",
       " 6588932,\n",
       " 543900,\n",
       " 2952913,\n",
       " 3755315,\n",
       " 8585898,\n",
       " 6734965,\n",
       " 6835114,\n",
       " 6768405,\n",
       " 1852755,\n",
       " 2688277,\n",
       " 837108,\n",
       " 4444340,\n",
       " 7256474,\n",
       " 4213924,\n",
       " 1237995,\n",
       " 1508528,\n",
       " 2003540,\n",
       " 3593822,\n",
       " 4845044,\n",
       " 6996107,\n",
       " 7006084,\n",
       " 2317637,\n",
       " 312915,\n",
       " 1636758,\n",
       " 3005841,\n",
       " 3525937,\n",
       " 6577487,\n",
       " 1310293,\n",
       " 2063341,\n",
       " 8370584,\n",
       " 8370580,\n",
       " 3146937,\n",
       " 4210525,\n",
       " 4554881,\n",
       " 2897523,\n",
       " 4444341,\n",
       " 1203016,\n",
       " 3933815,\n",
       " 4265296,\n",
       " 67809,\n",
       " 1052260,\n",
       " 3612177,\n",
       " 3224339,\n",
       " 3007383,\n",
       " 3041000,\n",
       " 4430985,\n",
       " 4555592,\n",
       " 4075450,\n",
       " 2331377,\n",
       " 2023579,\n",
       " 3186899,\n",
       " 6747151,\n",
       " 7006085,\n",
       " 7595911,\n",
       " 4514742,\n",
       " 8529159,\n",
       " 8090530,\n",
       " 1184445,\n",
       " 8449079,\n",
       " 8449080,\n",
       " 4498190,\n",
       " 3904770,\n",
       " 2956039,\n",
       " 2274920,\n",
       " 6132446,\n",
       " 2604147,\n",
       " 1439914,\n",
       " 1496506,\n",
       " 5488204,\n",
       " 6270154,\n",
       " 1508520,\n",
       " 7912172,\n",
       " 536321,\n",
       " 1184446,\n",
       " 4570849,\n",
       " 182442,\n",
       " 5229963,\n",
       " 6614218,\n",
       " 8594581,\n",
       " 739564,\n",
       " 7042206,\n",
       " 2105318,\n",
       " 8495820,\n",
       " 4818715,\n",
       " 8296635,\n",
       " 4570848,\n",
       " 7224958,\n",
       " 8777084,\n",
       " 4265298,\n",
       " 4514744,\n",
       " 5257502,\n",
       " 6413313,\n",
       " 6229720,\n",
       " 2688282,\n",
       " 4920173,\n",
       " 5229959,\n",
       " 6270153,\n",
       " 2773988,\n",
       " 7825161,\n",
       " 5046863,\n",
       " 5172493,\n",
       " 3212728,\n",
       " 3695275,\n",
       " 7795749,\n",
       " 4260315,\n",
       " 3755314,\n",
       " 4286286,\n",
       " 7043258,\n",
       " 6964021,\n",
       " 2268913,\n",
       " 632105,\n",
       " 6617112,\n",
       " 8350598,\n",
       " 5700645,\n",
       " 616280,\n",
       " 2684910,\n",
       " 5229962,\n",
       " 6692607,\n",
       " 3067204,\n",
       " 3441344,\n",
       " 4265299,\n",
       " 2460382,\n",
       " 167806,\n",
       " 7856455,\n",
       " 7202988,\n",
       " 5489673,\n",
       " 7579465,\n",
       " 2331376,\n",
       " 8797329,\n",
       " 2788096,\n",
       " 3186900,\n",
       " 5616933,\n",
       " 6588936,\n",
       " 7746988,\n",
       " 8296634,\n",
       " 8491931,\n",
       " 3375902,\n",
       " 5521839,\n",
       " 2821632,\n",
       " 3278143,\n",
       " 5071921,\n",
       " 6734960,\n",
       " 8612583,\n",
       " 2669072,\n",
       " 2888194,\n",
       " 7105624,\n",
       " 7507825,\n",
       " 412866,\n",
       " 3603635,\n",
       " 4896055,\n",
       " 3229216,\n",
       " 4286283,\n",
       " 874623,\n",
       " 2313233,\n",
       " 3918940,\n",
       " 595905,\n",
       " 6030991,\n",
       " 7217053,\n",
       " 5970718,\n",
       " 5019725,\n",
       " 2685336,\n",
       " 3478775,\n",
       " 2274928,\n",
       " 5054175,\n",
       " 4867944,\n",
       " 3041333,\n",
       " 3041332,\n",
       " 6413308,\n",
       " 4317220,\n",
       " 3375908,\n",
       " 4072444,\n",
       " 4120107,\n",
       " 174304,\n",
       " 7560397,\n",
       " 7618171,\n",
       " 473868,\n",
       " 1480902,\n",
       " 2684909,\n",
       " 5100189,\n",
       " 8190081,\n",
       " 4421497,\n",
       " 6808390,\n",
       " 316641,\n",
       " 791752,\n",
       " 3822410,\n",
       " 4514739,\n",
       " 5340712,\n",
       " 6105645,\n",
       " 3678294,\n",
       " 473869,\n",
       " 2688281,\n",
       " 7525668,\n",
       " 721371,\n",
       " 5962354,\n",
       " 2697993,\n",
       " 4786207,\n",
       " 1262912,\n",
       " 5210212,\n",
       " 2745472,\n",
       " 8094280,\n",
       " 5700646,\n",
       " 6314438,\n",
       " 323584,\n",
       " 4444338,\n",
       " 7136710,\n",
       " 7865864,\n",
       " 8142075,\n",
       " 214872,\n",
       " 3186906,\n",
       " 8152424,\n",
       " 7771812,\n",
       " 214719,\n",
       " 3524002,\n",
       " 2243801,\n",
       " 3492789,\n",
       " 5700644,\n",
       " 6413315,\n",
       " 699087,\n",
       " 2119633,\n",
       " 3677687,\n",
       " 4066032,\n",
       " 5226648,\n",
       " 5845591,\n",
       " 5906550,\n",
       " 2633194,\n",
       " 2660503,\n",
       " 6747143,\n",
       " 7507829,\n",
       " 546029,\n",
       " 3032211,\n",
       " 612287,\n",
       " 8409187,\n",
       " 6614212,\n",
       " 7244317,\n",
       " 7960305,\n",
       " 744679,\n",
       " 2423406,\n",
       " 4555589,\n",
       " 6620095,\n",
       " 4286285,\n",
       " 7440313,\n",
       " 494674,\n",
       " 699089,\n",
       " 2859340,\n",
       " 3653676,\n",
       " 4004882,\n",
       " 5574310,\n",
       " 5634806,\n",
       " 5634807,\n",
       " 5746554,\n",
       " 5810907,\n",
       " 6098628,\n",
       " 6344412,\n",
       " 6618483,\n",
       " 7974371,\n",
       " 352369,\n",
       " 3751221,\n",
       " 3755318,\n",
       " 6581582,\n",
       " 632101,\n",
       " 973931,\n",
       " 2393822,\n",
       " 2697986,\n",
       " 6542672,\n",
       " 8622000,\n",
       " 5054177,\n",
       " 7507832,\n",
       " 559381,\n",
       " 559385,\n",
       " 1062136,\n",
       " 4265302,\n",
       " 3271469,\n",
       " 7990209,\n",
       " 7388251,\n",
       " 352368,\n",
       " 2787660,\n",
       " 4969177,\n",
       " 5945754,\n",
       " 7618167,\n",
       " 4331886,\n",
       " 6964020,\n",
       " 2236456,\n",
       " 8047356,\n",
       " 1511697,\n",
       " 4011359,\n",
       " 6568413,\n",
       " 8612587,\n",
       " 3918944,\n",
       " 2104003,\n",
       " 2804710,\n",
       " 5700648,\n",
       " 6096159,\n",
       " 8837657,\n",
       " 366820,\n",
       " 2550636,\n",
       " 2634859,\n",
       " 5607911,\n",
       " 2697991,\n",
       " 3144066,\n",
       " 1310287,\n",
       " 5607912,\n",
       " 2853652,\n",
       " 4604752,\n",
       " 3579816,\n",
       " 845676,\n",
       " 1538309,\n",
       " 2753858,\n",
       " 3973417,\n",
       " 7700234,\n",
       " 8394999,\n",
       " 8659067,\n",
       " 785927,\n",
       " 943731,\n",
       " 2660508,\n",
       " 7507831,\n",
       " 5431955,\n",
       " 167804,\n",
       " 685052,\n",
       " 66402,\n",
       " 316636,\n",
       " 3313982,\n",
       " 5166146,\n",
       " 5788161,\n",
       " 7340305,\n",
       " 1140831,\n",
       " 3653680,\n",
       " 4072445,\n",
       " 4544927,\n",
       " 7802824,\n",
       " 739566,\n",
       " 2460380,\n",
       " 4018455,\n",
       " 8044749,\n",
       " 3686464,\n",
       " 6196325,\n",
       " 785919,\n",
       " 1237991,\n",
       " 3603634,\n",
       " 6749841,\n",
       " 1761478,\n",
       " 4477435,\n",
       " 5849682,\n",
       " 706375,\n",
       " 2921774,\n",
       " 1472620,\n",
       " 3612171,\n",
       " 5906553,\n",
       " 7896828,\n",
       " 1428217,\n",
       " 6568418,\n",
       " 8668794,\n",
       " 352371,\n",
       " 576850,\n",
       " 3234254,\n",
       " 3234252,\n",
       " 4426170,\n",
       " 5966133,\n",
       " 6617115,\n",
       " 7371576,\n",
       " 7489433,\n",
       " 3146931,\n",
       " 4554876,\n",
       " 5474172,\n",
       " 6808553,\n",
       " 7304206,\n",
       " 8806110,\n",
       " 41907,\n",
       " 943726,\n",
       " 4555591,\n",
       " 6891007,\n",
       " 5373230,\n",
       " 2684371,\n",
       " 5521837,\n",
       " 8463309,\n",
       " 529157,\n",
       " 2063344,\n",
       " 2602436,\n",
       " 6891009,\n",
       " 3845684,\n",
       " 8429853,\n",
       " 3944379,\n",
       " 4683143,\n",
       " 5914767,\n",
       " 8218732,\n",
       " 4668282,\n",
       " 491118,\n",
       " 3647654,\n",
       " 4342883,\n",
       " 7403931,\n",
       " 5932113,\n",
       " 9928,\n",
       " 294969,\n",
       " 1413680,\n",
       " 2343843,\n",
       " 3904771,\n",
       " 4672435,\n",
       " 6949223,\n",
       " 943727,\n",
       " 6891006,\n",
       " 8491928,\n",
       " 5865407,\n",
       " 4533144,\n",
       " 7246677,\n",
       " 973928,\n",
       " 1636757,\n",
       " 1990416,\n",
       " 7666728,\n",
       " 3705070,\n",
       " 7142909,\n",
       " 7256472,\n",
       " 2688280,\n",
       " 3603631,\n",
       " 6056801,\n",
       " 859065,\n",
       " 1102969,\n",
       " 2589020,\n",
       " 7476061,\n",
       " 7042205,\n",
       " 1773993,\n",
       " 2616108,\n",
       " 8190078,\n",
       " 6308583,\n",
       " 3114301,\n",
       " 1412136,\n",
       " 3517774,\n",
       " 4357284,\n",
       " 4554883,\n",
       " 4753677,\n",
       " 7618169,\n",
       " 8714928,\n",
       " 6725068,\n",
       " 6286063,\n",
       " 6749842,\n",
       " 7430200,\n",
       " 8193666,\n",
       " 1608964,\n",
       " 4072450,\n",
       " 1191029,\n",
       " 1361783,\n",
       " 1607636,\n",
       " 1704969,\n",
       " 2952909,\n",
       " 3612173,\n",
       " 5521836,\n",
       " 5856317,\n",
       " 5929478,\n",
       " 6491588,\n",
       " 1700277,\n",
       " 494671,\n",
       " 2317638,\n",
       " 5616938,\n",
       " 6047175,\n",
       " 5050095,\n",
       " 5830852,\n",
       " 900981,\n",
       " 3271475,\n",
       " 6428189,\n",
       " 8529013,\n",
       " 8665012,\n",
       " 4994125,\n",
       " 6617109,\n",
       " 44,\n",
       " 2830802,\n",
       " 4108354,\n",
       " 5392338,\n",
       " 721373,\n",
       " 1412135,\n",
       " 1412139,\n",
       " 2583279,\n",
       " 2928914,\n",
       " 3331014,\n",
       " 3904764,\n",
       " 4323677,\n",
       " 6160703,\n",
       " 6690466,\n",
       " 6879901,\n",
       " 8142074,\n",
       " 706009,\n",
       " 214881,\n",
       " 744680,\n",
       " 1439918,\n",
       " 8612590,\n",
       " 214717,\n",
       " 3460267,\n",
       " 3590177,\n",
       " 4362390,\n",
       " 7412621,\n",
       " 7798093,\n",
       " 7865376,\n",
       " 7922116,\n",
       " 8190077,\n",
       " 3884188,\n",
       " 1096038,\n",
       " 2304449,\n",
       " 4136074,\n",
       " 5104570,\n",
       " 3768399,\n",
       " 615834,\n",
       " 3366456,\n",
       " 239220,\n",
       " 333956,\n",
       " 1237767,\n",
       " 2790492,\n",
       " 3904763,\n",
       " 4198223,\n",
       " 6708199,\n",
       " 6996105,\n",
       " 7618162,\n",
       " 7791455,\n",
       " 7996757,\n",
       " 8592847,\n",
       " 8594584,\n",
       " 8697059,\n",
       " 8806114,\n",
       " 8148386,\n",
       " 6259986,\n",
       " 475286,\n",
       " 2554575,\n",
       " 3524005,\n",
       " 6860994,\n",
       " 6903795,\n",
       " 7121468,\n",
       " 561223,\n",
       " 2460386,\n",
       " 2543800,\n",
       " 2814040,\n",
       " 2905188,\n",
       " 2928911,\n",
       " 3071055,\n",
       " 3253875,\n",
       " 3284460,\n",
       " 4362388,\n",
       " 5365826,\n",
       " 5523541,\n",
       " 5700642,\n",
       " 7571825,\n",
       " 8670801,\n",
       " 362553,\n",
       " 554804,\n",
       " 3585532,\n",
       " 5982122,\n",
       " 8572684,\n",
       " 2604142,\n",
       " 4570853,\n",
       " 2633195,\n",
       " 1773991,\n",
       " 4812393,\n",
       " 8412862,\n",
       " 3449721,\n",
       " 6668764,\n",
       " 4635959,\n",
       " 2896932,\n",
       " 182435,\n",
       " 1183678,\n",
       " 1237771,\n",
       " 1650869,\n",
       " 3177974,\n",
       " 3186934,\n",
       " 3579251,\n",
       " 3973413,\n",
       " 4489368,\n",
       " 5010154,\n",
       " 5912480,\n",
       " 5945752,\n",
       " 6614216,\n",
       " 7084189,\n",
       " 8068805,\n",
       " 8503982,\n",
       " 4183634,\n",
       " 4593084,\n",
       " 790853,\n",
       " 4995085,\n",
       " 6210195,\n",
       " 1871828,\n",
       " 3459745,\n",
       " 5962826,\n",
       " 6827618,\n",
       " 441105,\n",
       " 1116868,\n",
       " 1636754,\n",
       " 1636755,\n",
       " 2448318,\n",
       " 2532916,\n",
       " 3295439,\n",
       " 3510394,\n",
       " 3612172,\n",
       " 7179899,\n",
       " 8350601,\n",
       " 6178811,\n",
       " 3212334,\n",
       " 7800478,\n",
       " 984557,\n",
       " 646104,\n",
       " 7001423,\n",
       " 4443719,\n",
       " 8154928,\n",
       " 1863280,\n",
       " 2470498,\n",
       " 2730674,\n",
       " 2793386,\n",
       " 3533792,\n",
       " 4554880,\n",
       " 6080051,\n",
       " 7618168,\n",
       " 8419657,\n",
       " 8615522,\n",
       " 473871,\n",
       " 2274923,\n",
       " 5097066,\n",
       " 2684373,\n",
       " 3524010,\n",
       " 5270637,\n",
       " 6263577,\n",
       " 4138531,\n",
       " 3174749,\n",
       " 4263232,\n",
       " 7907439,\n",
       " 323589,\n",
       " 868612,\n",
       " 2156289,\n",
       " 2892628,\n",
       " 3493342,\n",
       " 3590183,\n",
       " 3755316,\n",
       " 5157789,\n",
       " 5365833,\n",
       " 6138783,\n",
       " 6773063,\n",
       " 6996100,\n",
       " 7370175,\n",
       " 7527108,\n",
       " 8221050,\n",
       " 1511702,\n",
       " 3227489,\n",
       " 4997064,\n",
       " 8225815,\n",
       " 6908170,\n",
       " 3612170,\n",
       " 2271355,\n",
       " 2780208,\n",
       " 5256063,\n",
       " 5929479,\n",
       " 182434,\n",
       " 556959,\n",
       " 616282,\n",
       " 1413677,\n",
       " 1718703,\n",
       " 3904767,\n",
       " 4956509,\n",
       " 4969178,\n",
       " 5113254,\n",
       " 5300526,\n",
       " 6080046,\n",
       " 6610691,\n",
       " 6708194,\n",
       " 6996103,\n",
       " 8106161,\n",
       " 8202935,\n",
       " 8592850,\n",
       " 7007346,\n",
       " 330833,\n",
       " 2146205,\n",
       " 2541367,\n",
       " 4255278,\n",
       " 4503225,\n",
       " 6915796,\n",
       " 7261666,\n",
       " 3554709,\n",
       " 6452034,\n",
       " 7605811,\n",
       " 3579256,\n",
       " 3772131,\n",
       " 5528865,\n",
       " 5867401,\n",
       " 5896014,\n",
       " 7048062,\n",
       " 7126325,\n",
       " 7527107,\n",
       " 8024805,\n",
       " 5379483,\n",
       " 4891076,\n",
       " 7528811,\n",
       " 1650137,\n",
       " 4289674,\n",
       " 8372615,\n",
       " 6277997,\n",
       " 7198042,\n",
       " 2132292,\n",
       " 5270644,\n",
       " 182436,\n",
       " 1237775,\n",
       " 2602434,\n",
       " 3257575,\n",
       " 3482971,\n",
       " 3762458,\n",
       " 4845040,\n",
       " 5287329,\n",
       " 5703869,\n",
       " 7767470,\n",
       " 8148785,\n",
       " 8286488,\n",
       " 8594590,\n",
       " 5379484,\n",
       " 6891005,\n",
       " 6964027,\n",
       " 4357889,\n",
       " 7781300,\n",
       " 1806764]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SparseRetrievalModel(nn.Module):\n",
    "    def __init__(self, sparse_collection, top_k = 10):\n",
    "        super().__init__()\n",
    "        #self.shape = sparse_collection.sparse_vecs, sparse_collection.shape\n",
    "        self.crow = torch.nn.parameter.Parameter(sparse_collection.sparse_vecs[0], requires_grad=False)\n",
    "        self.indice = torch.nn.parameter.Parameter(sparse_collection.sparse_vecs[1], requires_grad=False)\n",
    "        self.values = torch.nn.parameter.Parameter(sparse_collection.sparse_vecs[2], requires_grad=False)\n",
    "        self.collection_matrix = None#torch.sparse_csr_tensor(self.crow, self.indice, self.values, sparse_collection.shape)\n",
    "        self.shape = sparse_collection.shape\n",
    "        self.top_k = top_k\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= x.squeeze()\n",
    "        print(x.shape)\n",
    "        if self.collection_matrix is None:\n",
    "            self.collection_matrix = torch.sparse_csr_tensor(self.crow, self.indice, self.values, self.shape)\n",
    "       \n",
    "        return torch.topk(self.collection_matrix @ x, k=self.top_k, dim=0)#.cpu()\n",
    "        #return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retrieval_model = SparseRetrievalModel(sparse_collection, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../syn-question-col-analysis/question_generation/gen_output/msmarco/selected_corpus_lm_fcm_STD2_L10000_gpt-neo-1.3B_BS_5_E13931.459746599197.jsonl\") as f:\n",
    "    questions = [line for line in map(json.loads, f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_dense_torch(text):\n",
    "    b = bow(text)\n",
    "    return torch.sparse_coo_tensor([list(b.keys())], list(b.values()), (vocab_size,), dtype=torch.float32).to_dense()\n",
    "\n",
    "def text_to_sparse_torch(text):\n",
    "    b = bow(text)\n",
    "    return torch.sparse_coo_tensor([list(b.keys())], list(b.values()), (vocab_size,), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a non cpu device, but got: cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m retrieval_model_gpu \u001b[39m=\u001b[39m retrieval_model\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m retrieval_model_multigpu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mDataParallel(retrieval_model_gpu, output_device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:142\u001b[0m, in \u001b[0;36mDataParallel.__init__\u001b[0;34m(self, module, device_ids, output_device, dim)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule \u001b[39m=\u001b[39m module\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids \u001b[39m=\u001b[39m [_get_device_index(x, \u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m device_ids]\n\u001b[0;32m--> 142\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device \u001b[39m=\u001b[39m _get_device_index(output_device, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(device_type, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[\u001b[39m0\u001b[39m])\n\u001b[1;32m    145\u001b[0m _check_balance(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:715\u001b[0m, in \u001b[0;36m_get_device_index\u001b[0;34m(device, optional, allow_cpu)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, torch\u001b[39m.\u001b[39mdevice):\n\u001b[1;32m    714\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_cpu \u001b[39mand\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 715\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected a non cpu device, but got: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(device))\n\u001b[1;32m    716\u001b[0m     device_idx \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m device\u001b[39m.\u001b[39mindex\n\u001b[1;32m    717\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a non cpu device, but got: cpu"
     ]
    }
   ],
   "source": [
    "retrieval_model_gpu = retrieval_model.to(\"cuda\")\n",
    "retrieval_model_multigpu = torch.nn.DataParallel(retrieval_model_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1170682])\n",
      "torch.Size([2, 1170682])\n"
     ]
    }
   ],
   "source": [
    "query1 = text_to_dense_torch(questions[0][\"question\"])#.to(\"cuda:0\")\n",
    "print(query1.shape)\n",
    "query2 = text_to_dense_torch(questions[1][\"question\"])\n",
    "query = torch.stack([query1, query2])\n",
    "query = query#.to(\"cuda\")\n",
    "print(query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1170682])\n",
      "torch.Size([1170682])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Caught AttributeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tiagoalmeida/.local/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/tiagoalmeida/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_277486/216961384.py\", line 20, in forward\n    return torch.topk(self.collection_matrix @ x, k=self.top_k, dim=0).cpu()\nAttributeError: 'torch.return_types.topk' object has no attribute 'cpu'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[39m=\u001b[39m retrieval_model_multigpu(query)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[39m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         output\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m     90\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mAttributeError\u001b[0m: Caught AttributeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tiagoalmeida/.local/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/tiagoalmeida/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_277486/216961384.py\", line 20, in forward\n    return torch.topk(self.collection_matrix @ x, k=self.top_k, dim=0).cpu()\nAttributeError: 'torch.return_types.topk' object has no attribute 'cpu'\n"
     ]
    }
   ],
   "source": [
    "out = retrieval_model_multigpu(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([73.5256, 43.7951, 43.2043, 41.2657, 40.5497, 40.2520, 38.8254, 38.5653,\n",
       "        37.7857, 35.0274, 73.5256, 43.7951, 43.2043, 41.2657, 40.5497, 40.2520,\n",
       "        38.8254, 38.5653, 35.0274, 34.3543], device='cuda:0'),\n",
       "indices=tensor([    341, 6080044, 6080053, 6080050, 2787655, 3224317, 4383297, 5254910,\n",
       "        3005850, 8529163,     341, 6080044, 6080053, 6080050, 2787655, 3224317,\n",
       "        4383297, 5254910, 8529163, 2684918], device='cuda:0'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out\n",
    "\n",
    "#print(1000/(end_t-start_t), \"it/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/748 [00:18<14:07,  1.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m bq \u001b[39min\u001b[39;00m tqdm(batches_questions):\n\u001b[0;32m----> 6\u001b[0m     results\u001b[39m.\u001b[39mappend(batch_retrieve_topk(bq,\u001b[39m10\u001b[39;49m))\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mbatch_retrieve_topk\u001b[0;34m(questions_text, at)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m i, question \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(questions_text):\n\u001b[1;32m     28\u001b[0m     queries_gpu_tensor[:,i] \u001b[39m=\u001b[39m text_to_dense_torch(question)\n\u001b[0;32m---> 30\u001b[0m queries_gpu_tensor \u001b[39m=\u001b[39m queries_gpu_tensor\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda:1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     32\u001b[0m end_stack \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     33\u001b[0m r \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtopk(csr_matrix_gpu \u001b[39m@\u001b[39m queries_gpu_tensor, k\u001b[39m=\u001b[39mat, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# batch retrievel\n",
    "batches_questions = [list(map(lambda x:x[\"question\"], questions[i:i+64])) for i in range(64,len(questions),64)]\n",
    "\n",
    "results = []\n",
    "for bq in tqdm(batches_questions):\n",
    "    results.append(batch_retrieve_topk(bq,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batches_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gpu = text_to_dense_torch(questions[0][\"question\"]).to(\"cuda:0\")\n",
    "r = torch.topk(csr_matrix_gpu @ query_gpu, k=10, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([50.9797, 30.3736, 29.9639, 28.6196, 28.1220, 27.9136, 26.9275, 26.7314,\n",
       "        26.2070, 24.2917], device='cuda:0'),\n",
       "indices=tensor([    341, 6080044, 6080053, 6080050, 2787655, 3224317, 4383297, 5254910,\n",
       "        3005850, 8529163], device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6080050"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_collection.metadata.index2docID[\"6080053\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {880134: 1.0, 869173: 1.0, 274027: 1.0, 541942: 1.0, 652667: 1.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow(questions[0][\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relationship', 'rarotongan', 'cook', 'island', 'maori']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tokens = pipe(pd.DataFrame([{\"qid\":0, \"query\":questions[0][\"question\"].lower()}]))[\"query\"][0]\n",
    "query_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MSMARCO = \"../syn-question-col-analysis/datasets/msmarco/corpus_L8841823.jsonl\"\n",
    "\n",
    "def get_title_abstract(string):\n",
    "    data = json.loads(string)\n",
    "    title, abstract = data[\"title\"], data[\"abstract\"]\n",
    "    return f\"{title} {abstract}\"\n",
    "\n",
    "with open(PATH_TO_MSMARCO) as f:\n",
    "    for doc in map(get_title_abstract, f):\n",
    "       \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {839186: 1.0,\n",
       "             266051: 1.0,\n",
       "             927074: 2.0,\n",
       "             684233: 1.0,\n",
       "             373151: 1.0,\n",
       "             523132: 1.0,\n",
       "             998799: 2.0,\n",
       "             651144: 1.0,\n",
       "             845056: 1.0,\n",
       "             533255: 1.0,\n",
       "             767761: 1.0,\n",
       "             257234: 1.0,\n",
       "             474305: 1.0,\n",
       "             523347: 1.0,\n",
       "             77832: 1.0,\n",
       "             140132: 1.0,\n",
       "             884129: 1.0,\n",
       "             367894: 1.0,\n",
       "             1066913: 1.0,\n",
       "             666305: 1.0,\n",
       "             506868: 1.0,\n",
       "             1042085: 1.0,\n",
       "             530548: 1.0,\n",
       "             625474: 1.0,\n",
       "             754559: 1.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "batched_queries = [questions[i:i + batch_size] for i in range(0, len(questions), batch_size)]\n",
    "\n",
    "results = []\n",
    "start_t = time.time()\n",
    "for batch in tqdm(batched_queries):\n",
    "    results.append(batch_retrieve_topk(list(map(lambda x:x[\"question\"], batch))))\n",
    "end_t = time.time()\n",
    "\n",
    "print(\"took\", end_t-start_t)\n",
    "# 0.003 - 0.003\n",
    "# 0.008 - 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gpu = text_to_dense_torch(question[\"question\"]).to(\"cuda:1\")\n",
    "end_stack = time.time()\n",
    "r = torch.topk(csr_matrix_gpu @ query_gpu, k=10, dim=0).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1424203, 2678308, 3748774, 3233037, 2741266, 5938529, 5026090, 5495619,\n",
       "        2542788,  838903], device='cuda:1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.to_sparse_csc>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_dense_torch(text, dim):\n",
    "    b = bow(text)\n",
    "    return torch.sparse_coo_tensor([list(b.keys())], list(b.values()), (dim,), dtype=torch.float32).to_dense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_matrix_gpu = csr_matrix_cpu.to(\"cuda:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_QUESTIONS = \"../syn-question-col-analysis/datasets/msmarco/relevant_pairs.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(PATH_TO_QUESTIONS) as f:\n",
    "    questions = {line[\"question\"] for line in map(json.loads,f)}\n",
    "\n",
    "questions = [text_to_dense_torch(q, csr_matrix_cpu.shape[1]) for q in questions]\n",
    "queries_gpu = torch.stack(questions[:20], -1).to(\"cuda:1\")\n",
    "\n",
    "#sort, idx = (csr_matrix_gpu @ queries_gpu).sort(descending=True, dim=0)\n",
    "#result = idx[:10].cpu()\n",
    "result = torch.topk(csr_matrix_gpu @ queries_gpu, k=10, dim=0).indices.T.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8617271, 7607669, 5466810, 1379245, 1379240, 5466807, 1664523, 8617274,\n",
       "          547444,  269428],\n",
       "        [1929910, 3572695, 7839904, 1288938, 4842897, 2507917, 7839906, 3572702,\n",
       "         5359212, 3937200],\n",
       "        [2533260, 5012351, 8121380,  719552, 5291683, 7952865, 5291686,  719550,\n",
       "         6528714, 7088568],\n",
       "        [8049577, 1433123,  669004, 4563960, 1584254, 3410067, 2055598, 8049578,\n",
       "           16845, 3552218],\n",
       "        [8635981, 7267248,  527698, 3260688, 1837110, 1958102, 1958100, 7267243,\n",
       "         8199361, 7367407],\n",
       "        [8760867, 8760864, 3641634, 2787508, 4788864, 2157456, 8760868, 8760873,\n",
       "         3620983, 3342992],\n",
       "        [7778351, 2868845, 6436703, 7778348, 4164404, 4337532, 2197526, 2997653,\n",
       "         7670593, 2160853],\n",
       "        [7447941, 8433858, 6654655, 8433854, 7896211, 2747492, 2365660, 5638740,\n",
       "         4704978, 2702419],\n",
       "        [8160520, 3838645,  554521, 4511137,  398442, 4575877, 5218014, 8160527,\n",
       "         1901881, 8478604],\n",
       "        [ 536176, 3705165, 2978866, 5653659, 2329697, 4511503, 4606387, 5653652,\n",
       "         1006868, 1006859],\n",
       "        [1445087, 8446502, 1798612, 6177939, 5699286, 8093934, 8446505, 8714082,\n",
       "         5752877, 3349612],\n",
       "        [5674622, 8255705,  190809, 8160241, 1883281, 3953805, 6467522, 4443375,\n",
       "         5674623, 7093509],\n",
       "        [1749882, 8819113, 7965004, 6018163, 8081937, 2026777, 7843984, 8819111,\n",
       "          186939, 4634899],\n",
       "        [ 576629,   68647,   68653,   68648, 2293820,  943229,   68649, 2826510,\n",
       "         3223833, 3223831],\n",
       "        [8128798, 6021898, 8128790, 4981632, 3997986, 8128796, 1066532, 8128794,\n",
       "         7618564, 4486954],\n",
       "        [8273755, 8273763, 7941579, 8273762, 8273759, 6031630, 2998723, 8273757,\n",
       "         8273754, 8185064],\n",
       "        [7911557, 8588219, 8588222, 8588226, 2697752, 2697746, 8588227,  128984,\n",
       "         2176863,  302210],\n",
       "        [1334335, 5836920, 8348781, 7726655, 8084332, 3537614, 8199524, 2143418,\n",
       "         2777108, 2777113],\n",
       "        [ 115142, 8683095, 6230226, 1524406, 2702357, 3437288, 6230227, 1524402,\n",
       "         1969741, 1900577],\n",
       "        [8117094, 8117087, 6704400, 8117093, 7196961, 8117092, 6704401, 5279567,\n",
       "         8327123, 5139286]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del queries_gpu\n",
    "del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_list = []\n",
    "for _ in range(20):\n",
    "    \n",
    "    with open(PATH_TO_QUESTIONS) as f:\n",
    "        questions = {line[\"question\"] for line in map(json.loads,f)}\n",
    "\n",
    "    questions = [text_to_dense_torch(q, csr_matrix_cpu.shape[1]) for q in questions]\n",
    "    queries_gpu = torch.stack(questions, -1).to(\"cuda:1\")\n",
    "\n",
    "    start_t = time.time()\n",
    "    #sort, idx = (csr_matrix_gpu @ queries_gpu).sort(descending=True, dim=0)\n",
    "    #result = idx[:10].cpu()\n",
    "    result = torch.topk(csr_matrix_gpu @ queries_gpu, k=100000, dim=0).indices.cpu()\n",
    "    time_list.append(time.time()-start_t)\n",
    "    \n",
    "    for q in questions:\n",
    "        del q\n",
    "    del questions\n",
    "    del queries_gpu\n",
    "    del result \n",
    "    #del sort\n",
    "    #del idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4787881374359131,\n",
       " 0.44898533821105957,\n",
       " 0.4485352039337158,\n",
       " 0.4484899044036865,\n",
       " 0.44817113876342773,\n",
       " 0.4484434127807617,\n",
       " 0.4484429359436035,\n",
       " 0.44898128509521484,\n",
       " 0.44836854934692383,\n",
       " 0.44843435287475586,\n",
       " 0.4483344554901123,\n",
       " 0.4485807418823242,\n",
       " 0.44898390769958496,\n",
       " 0.44857192039489746,\n",
       " 0.44869017601013184,\n",
       " 0.448455810546875,\n",
       " 0.44840002059936523,\n",
       " 0.4485960006713867,\n",
       " 0.4486415386199951,\n",
       " 0.4485054016113281]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_matrix_gpu = csr_matrix_cpu.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [text_to_dense_torch(\"what is the meaning of life?\", csr_matrix_cpu.shape[1]),\n",
    "           text_to_dense_torch(\"what time it is?\", csr_matrix_cpu.shape[1]),\n",
    "            ]\n",
    "\n",
    "queries = torch.stack(queries, -1)\n",
    "queries_gpu = queries.to(\"cuda:1\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.3629, 3.6962],\n",
       "        [0.5774, 0.0000],\n",
       "        [0.5461, 1.5242],\n",
       "        ...,\n",
       "        [0.5942, 0.0000],\n",
       "        [0.5746, 2.5969],\n",
       "        [1.1826, 0.5707]], device='cuda:1')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time_list = []\n",
    "for _ in range(20):\n",
    "    \n",
    "    queries = [text_to_dense_torch(\"what is the meaning of life?\", csr_matrix_cpu.shape[1]),\n",
    "           text_to_dense_torch(\"what time it is?\", csr_matrix_cpu.shape[1]),\n",
    "           text_to_dense_torch(\"what time it is? 2\", csr_matrix_cpu.shape[1]),\n",
    "           text_to_dense_torch(\"wagfsdf asdg asg ?\", csr_matrix_cpu.shape[1]),\n",
    "           #text_to_dense_torch(\"wwef faes ewq ta afewf s?\", csr_matrix_cpu.shape[1]),\n",
    "            ]\n",
    "\n",
    "    queries = torch.stack(queries, -1)\n",
    "    queries_gpu = queries.to(\"cuda:1\")\n",
    "\n",
    "    start_t = time.time()\n",
    "    csr_matrix_gpu @ queries_gpu\n",
    "    time_list.append(time.time()-start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08742809295654297"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_list)/len(time_list)*1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(PATH_TO_QUESTIONS) as f:\n",
    "    questions = {line[\"question\"] for line in map(json.loads,f)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_gpu = queries.to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.42 GiB (GPU 1; 7.79 GiB total capacity; 6.26 GiB already allocated; 1.19 GiB free; 6.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m queries_gpu \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([text_to_dense_torch(q, csr_matrix_cpu\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m questions], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda:1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m start_t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m result \u001b[39m=\u001b[39m csr_matrix_gpu \u001b[39m@\u001b[39;49m queries_gpu\n\u001b[1;32m     12\u001b[0m time_list\u001b[39m.\u001b[39mappend(time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart_t)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.42 GiB (GPU 1; 7.79 GiB total capacity; 6.26 GiB already allocated; 1.19 GiB free; 6.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1820206642150879"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(time_list)/len(time_list)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%timeit -n 1000\n",
    "retrieve_gpu_nvprims_nvfuser(query_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.3629, 0.5774, 0.5461,  ..., 0.5942, 0.5746, 1.1826], device='cuda:1')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_gpu = query.to(\"cuda:1\")\n",
    "retrieve_gpu4(csr_matrix_gpu, query_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 ms ± 78.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -l 1000\n",
    "retrieve_gpu4(csr_matrix_gpu, query_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 ms ± 88 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "csr_matrix @ query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 ms ± 4.17 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.topk(csr_matrix@query,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30522])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([     0,     40,     67,  ..., 465841, 465881,\n",
       "                            465917]),\n",
       "       col_indices=tensor([ 1012,  1025,  1996,  ...,  9949, 17502, 26110]),\n",
       "       values=tensor([2., 1., 6.,  ..., 1., 1., 3.]), size=(10000, 30522),\n",
       "       nnz=465917, layout=torch.sparse_csr)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to BM25 matrix: 100%|██████████| 10000/10000 [00:00<00:00, 27992.58it/s]\n"
     ]
    }
   ],
   "source": [
    "sparseCSR_collection.transform(BM25Transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([     0,     40,     67,  ..., 465841, 465881, 465917],\n",
       "        dtype=torch.int32),\n",
       " tensor([ 1012,  1025,  1996,  ...,  9949, 17502, 26110], dtype=torch.int32),\n",
       " tensor([0.0000, 2.1513, 0.0000,  ..., 6.4540, 8.6054, 9.9709]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparseCSR_collection.sparse_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(crow_indices=tensor([     0,     40,     67,  ..., 465841, 465881,\n",
       "                            465917]),\n",
       "       col_indices=tensor([ 1012,  1025,  1996,  ...,  9949, 17502, 26110]),\n",
       "       values=tensor([0.0000, 2.1513, 0.0000,  ..., 6.4540, 8.6054, 9.9709]),\n",
       "       size=(10000, 30522), nnz=465917, layout=torch.sparse_csr)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sparse_csr_tensor(*sparseCSR_collection.sparse_vecs, sparseCSR_collection.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparse-retrieval-update",
   "language": "python",
   "name": "sparse-retrieval-update"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
